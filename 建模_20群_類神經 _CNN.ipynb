{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "from keras.datasets import reuters\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_validate\n",
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam,RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier \n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1680 entries, 0 to 1686\n",
      "Data columns (total 8 columns):\n",
      "age                1680 non-null float64\n",
      "serveTime          1680 non-null float64\n",
      "credLimit          1680 non-null int64\n",
      "Loan               1680 non-null float64\n",
      "SalPerY            1680 non-null int64\n",
      "holdCard           1680 non-null int64\n",
      "Career             1680 non-null int64\n",
      "credLimit_group    1680 non-null int32\n",
      "dtypes: float64(3), int32(1), int64(4)\n",
      "memory usage: 111.6 KB\n"
     ]
    }
   ],
   "source": [
    "#開檔\n",
    "df = pd.read_excel(r'C:\\Users\\Big data\\Desktop\\class\\funcardproject\\斷詞與和卡額度_20群.xls',encoding='utf-16')\n",
    "df = df.loc[:, [\"age\",\"serveTime\",\"credLimit\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\",\"credLimit_group\"]] \n",
    "#若某raw有NAN則整RAW刪除\n",
    "df =df.dropna(\n",
    "    axis=0,     # 0: 对行进行操作; 1: 对列进行操作\n",
    "    how='any'   # 'any': 只要存在 NaN 就 drop 掉; 'all': 必须全部是 NaN 才 drop \n",
    "    ) \n",
    "#把分群的Y轉成int\n",
    "df['credLimit_group'] = df['credLimit_group'].astype('int')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1176, 6)\n",
      "(504, 6)\n",
      "(1176, 1)\n",
      "(504, 1)\n"
     ]
    }
   ],
   "source": [
    "#先打散資料(三次)\n",
    "for i in range(3):\n",
    "    df = shuffle(df)\n",
    "#再切成訓練與測試\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(df.loc[:, [\"age\",\"serveTime\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\"]] , df.loc[:, [\"credLimit_group\"]] , test_size=0.3, random_state=42)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_targets.shape)\n",
    "print(test_targets.shape)\n",
    "#轉array\n",
    "train_data = np.array(train_data).astype(float)\n",
    "test_data = np.array(test_data).astype(float)\n",
    "train_targets = np.array(train_targets).astype(int)\n",
    "test_targets = np.array(test_targets).astype(int)\n",
    "#把Y弄成onehot\n",
    "def to_one_hot(labels, dimension=20):\n",
    "    results = np.zeros((len(labels),dimension))\n",
    "    for i,label in enumerate(labels):\n",
    "        results[i,label]=1.\n",
    "    return results\n",
    "train_targets = to_one_hot(train_targets)\n",
    "test_targets = to_one_hot(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1176, 6)\n",
      "(504, 6)\n",
      "(1176, 20)\n",
      "(504, 20)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_targets.shape)\n",
    "print(test_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正規化(例如本資料serveTime與SalPerY不同單位且數值差異甚大,因此需轉為標準差)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 標準化\n",
    "# mean = train_data.mean(axis=0)\n",
    "# train_data -=mean\n",
    "# std = train_data.std(axis=0)\n",
    "# train_data/=std\n",
    "# test_data-=mean\n",
    "# test_data/=std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正規化\n",
    "#因為relu,所以這個比較好\n",
    "train_data_max = train_data.max(axis=0)\n",
    "train_data_min = train_data.min(axis=0)\n",
    "train_data_range = train_data_max-train_data_min\n",
    "train_data-=train_data_min\n",
    "train_data/=train_data_range\n",
    "\n",
    "test_data_max = test_data.max(axis=0)\n",
    "test_data_min = test_data.min(axis=0)\n",
    "test_data_range = test_data_max-test_data_min\n",
    "test_data-=test_data_min\n",
    "test_data/=test_data_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 6, 6)              36        \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1, 6)              222       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                140       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 398\n",
      "Trainable params: 398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(6,6,input_length=6))\n",
    "model.add(layers.Conv1D(6,6,activation='relu'))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(20))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1176 samples, validate on 504 samples\n",
      "Epoch 1/800\n",
      "1176/1176 [==============================] - 0s 93us/step - loss: 1.9743 - accuracy: 0.2764 - val_loss: 1.9881 - val_accuracy: 0.2560\n",
      "Epoch 2/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9734 - accuracy: 0.2764 - val_loss: 1.9879 - val_accuracy: 0.2560\n",
      "Epoch 3/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9724 - accuracy: 0.2764 - val_loss: 1.9878 - val_accuracy: 0.2560\n",
      "Epoch 4/800\n",
      "1176/1176 [==============================] - 0s 10us/step - loss: 1.9716 - accuracy: 0.2764 - val_loss: 1.9877 - val_accuracy: 0.2560\n",
      "Epoch 5/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9709 - accuracy: 0.2764 - val_loss: 1.9877 - val_accuracy: 0.2560\n",
      "Epoch 6/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9704 - accuracy: 0.2764 - val_loss: 1.9874 - val_accuracy: 0.2560\n",
      "Epoch 7/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9698 - accuracy: 0.2764 - val_loss: 1.9874 - val_accuracy: 0.2560\n",
      "Epoch 8/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9692 - accuracy: 0.2764 - val_loss: 1.9873 - val_accuracy: 0.2560\n",
      "Epoch 9/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9688 - accuracy: 0.2764 - val_loss: 1.9874 - val_accuracy: 0.2560\n",
      "Epoch 10/800\n",
      "1176/1176 [==============================] - 0s 15us/step - loss: 1.9683 - accuracy: 0.2764 - val_loss: 1.9874 - val_accuracy: 0.2560\n",
      "Epoch 11/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9680 - accuracy: 0.2764 - val_loss: 1.9873 - val_accuracy: 0.2560\n",
      "Epoch 12/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9675 - accuracy: 0.2764 - val_loss: 1.9877 - val_accuracy: 0.2560\n",
      "Epoch 13/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9671 - accuracy: 0.2764 - val_loss: 1.9879 - val_accuracy: 0.2560\n",
      "Epoch 14/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9668 - accuracy: 0.2764 - val_loss: 1.9879 - val_accuracy: 0.2560\n",
      "Epoch 15/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9665 - accuracy: 0.2764 - val_loss: 1.9881 - val_accuracy: 0.2560\n",
      "Epoch 16/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9659 - accuracy: 0.2764 - val_loss: 1.9879 - val_accuracy: 0.2560\n",
      "Epoch 17/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9658 - accuracy: 0.2764 - val_loss: 1.9878 - val_accuracy: 0.2560\n",
      "Epoch 18/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9654 - accuracy: 0.2764 - val_loss: 1.9880 - val_accuracy: 0.2560\n",
      "Epoch 19/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9651 - accuracy: 0.2764 - val_loss: 1.9882 - val_accuracy: 0.2560\n",
      "Epoch 20/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9650 - accuracy: 0.2764 - val_loss: 1.9881 - val_accuracy: 0.2560\n",
      "Epoch 21/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9646 - accuracy: 0.2764 - val_loss: 1.9884 - val_accuracy: 0.2560\n",
      "Epoch 22/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9645 - accuracy: 0.2764 - val_loss: 1.9887 - val_accuracy: 0.2560\n",
      "Epoch 23/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9641 - accuracy: 0.2764 - val_loss: 1.9888 - val_accuracy: 0.2560\n",
      "Epoch 24/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9639 - accuracy: 0.2764 - val_loss: 1.9888 - val_accuracy: 0.2560\n",
      "Epoch 25/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9638 - accuracy: 0.2764 - val_loss: 1.9889 - val_accuracy: 0.2560\n",
      "Epoch 26/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9637 - accuracy: 0.2764 - val_loss: 1.9890 - val_accuracy: 0.2560\n",
      "Epoch 27/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9633 - accuracy: 0.2764 - val_loss: 1.9890 - val_accuracy: 0.2560\n",
      "Epoch 28/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9633 - accuracy: 0.2764 - val_loss: 1.9890 - val_accuracy: 0.2560\n",
      "Epoch 29/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9630 - accuracy: 0.2764 - val_loss: 1.9892 - val_accuracy: 0.2560\n",
      "Epoch 30/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9629 - accuracy: 0.2764 - val_loss: 1.9894 - val_accuracy: 0.2560\n",
      "Epoch 31/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9628 - accuracy: 0.2755 - val_loss: 1.9894 - val_accuracy: 0.2560\n",
      "Epoch 32/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9625 - accuracy: 0.2764 - val_loss: 1.9896 - val_accuracy: 0.2560\n",
      "Epoch 33/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9624 - accuracy: 0.2764 - val_loss: 1.9900 - val_accuracy: 0.2560\n",
      "Epoch 34/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9622 - accuracy: 0.2764 - val_loss: 1.9900 - val_accuracy: 0.2560\n",
      "Epoch 35/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9621 - accuracy: 0.2764 - val_loss: 1.9901 - val_accuracy: 0.2560\n",
      "Epoch 36/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9619 - accuracy: 0.2713 - val_loss: 1.9899 - val_accuracy: 0.2401\n",
      "Epoch 37/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9618 - accuracy: 0.2806 - val_loss: 1.9899 - val_accuracy: 0.2401\n",
      "Epoch 38/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9617 - accuracy: 0.2806 - val_loss: 1.9898 - val_accuracy: 0.2401\n",
      "Epoch 39/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9615 - accuracy: 0.2806 - val_loss: 1.9902 - val_accuracy: 0.2401\n",
      "Epoch 40/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9615 - accuracy: 0.2806 - val_loss: 1.9903 - val_accuracy: 0.2401\n",
      "Epoch 41/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9612 - accuracy: 0.2806 - val_loss: 1.9906 - val_accuracy: 0.2401\n",
      "Epoch 42/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9611 - accuracy: 0.2806 - val_loss: 1.9906 - val_accuracy: 0.2401\n",
      "Epoch 43/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9610 - accuracy: 0.2806 - val_loss: 1.9905 - val_accuracy: 0.2401\n",
      "Epoch 44/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9609 - accuracy: 0.2687 - val_loss: 1.9906 - val_accuracy: 0.2560\n",
      "Epoch 45/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9608 - accuracy: 0.2636 - val_loss: 1.9907 - val_accuracy: 0.2401\n",
      "Epoch 46/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9607 - accuracy: 0.2798 - val_loss: 1.9908 - val_accuracy: 0.2560\n",
      "Epoch 47/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9606 - accuracy: 0.2747 - val_loss: 1.9909 - val_accuracy: 0.2401\n",
      "Epoch 48/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9606 - accuracy: 0.2721 - val_loss: 1.9909 - val_accuracy: 0.2401\n",
      "Epoch 49/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9605 - accuracy: 0.2806 - val_loss: 1.9909 - val_accuracy: 0.2401\n",
      "Epoch 50/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9604 - accuracy: 0.2806 - val_loss: 1.9910 - val_accuracy: 0.2401\n",
      "Epoch 51/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9601 - accuracy: 0.2806 - val_loss: 1.9911 - val_accuracy: 0.2401\n",
      "Epoch 52/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9603 - accuracy: 0.2806 - val_loss: 1.9910 - val_accuracy: 0.2401\n",
      "Epoch 53/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9599 - accuracy: 0.2806 - val_loss: 1.9912 - val_accuracy: 0.2401\n",
      "Epoch 54/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9598 - accuracy: 0.2806 - val_loss: 1.9914 - val_accuracy: 0.2401\n",
      "Epoch 55/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9598 - accuracy: 0.2806 - val_loss: 1.9917 - val_accuracy: 0.2401\n",
      "Epoch 56/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9598 - accuracy: 0.2806 - val_loss: 1.9917 - val_accuracy: 0.2401\n",
      "Epoch 57/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9596 - accuracy: 0.2806 - val_loss: 1.9917 - val_accuracy: 0.2401\n",
      "Epoch 58/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9596 - accuracy: 0.2806 - val_loss: 1.9915 - val_accuracy: 0.2401\n",
      "Epoch 59/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9595 - accuracy: 0.2806 - val_loss: 1.9916 - val_accuracy: 0.2401\n",
      "Epoch 60/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9594 - accuracy: 0.2806 - val_loss: 1.9916 - val_accuracy: 0.2401\n",
      "Epoch 61/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9593 - accuracy: 0.2806 - val_loss: 1.9915 - val_accuracy: 0.2401\n",
      "Epoch 62/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9593 - accuracy: 0.2806 - val_loss: 1.9917 - val_accuracy: 0.2401\n",
      "Epoch 63/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9591 - accuracy: 0.2806 - val_loss: 1.9917 - val_accuracy: 0.2401\n",
      "Epoch 64/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9591 - accuracy: 0.2806 - val_loss: 1.9917 - val_accuracy: 0.2401\n",
      "Epoch 65/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9590 - accuracy: 0.2806 - val_loss: 1.9918 - val_accuracy: 0.2401\n",
      "Epoch 66/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9590 - accuracy: 0.2806 - val_loss: 1.9919 - val_accuracy: 0.2401\n",
      "Epoch 67/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9588 - accuracy: 0.2806 - val_loss: 1.9920 - val_accuracy: 0.2401\n",
      "Epoch 68/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9589 - accuracy: 0.2806 - val_loss: 1.9922 - val_accuracy: 0.2401\n",
      "Epoch 69/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9588 - accuracy: 0.2806 - val_loss: 1.9923 - val_accuracy: 0.2401\n",
      "Epoch 70/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9588 - accuracy: 0.2806 - val_loss: 1.9925 - val_accuracy: 0.2401\n",
      "Epoch 71/800\n",
      "1176/1176 [==============================] - 0s 16us/step - loss: 1.9587 - accuracy: 0.2806 - val_loss: 1.9927 - val_accuracy: 0.2401\n",
      "Epoch 72/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9585 - accuracy: 0.2806 - val_loss: 1.9926 - val_accuracy: 0.2401\n",
      "Epoch 73/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9585 - accuracy: 0.2806 - val_loss: 1.9924 - val_accuracy: 0.2401\n",
      "Epoch 74/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9584 - accuracy: 0.2806 - val_loss: 1.9926 - val_accuracy: 0.2401\n",
      "Epoch 75/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9584 - accuracy: 0.2806 - val_loss: 1.9925 - val_accuracy: 0.2401\n",
      "Epoch 76/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9583 - accuracy: 0.2806 - val_loss: 1.9925 - val_accuracy: 0.2401\n",
      "Epoch 77/800\n",
      "1176/1176 [==============================] - 0s 10us/step - loss: 1.9583 - accuracy: 0.2806 - val_loss: 1.9928 - val_accuracy: 0.2401\n",
      "Epoch 78/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9581 - accuracy: 0.2806 - val_loss: 1.9929 - val_accuracy: 0.2401\n",
      "Epoch 79/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9585 - accuracy: 0.2806 - val_loss: 1.9929 - val_accuracy: 0.2401\n",
      "Epoch 80/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9581 - accuracy: 0.2806 - val_loss: 1.9928 - val_accuracy: 0.2401\n",
      "Epoch 81/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9580 - accuracy: 0.2806 - val_loss: 1.9931 - val_accuracy: 0.2401\n",
      "Epoch 82/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9580 - accuracy: 0.2806 - val_loss: 1.9932 - val_accuracy: 0.2401\n",
      "Epoch 83/800\n",
      "1176/1176 [==============================] - 0s 15us/step - loss: 1.9579 - accuracy: 0.2806 - val_loss: 1.9934 - val_accuracy: 0.2401\n",
      "Epoch 84/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9579 - accuracy: 0.2806 - val_loss: 1.9935 - val_accuracy: 0.2401\n",
      "Epoch 85/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9578 - accuracy: 0.2806 - val_loss: 1.9933 - val_accuracy: 0.2401\n",
      "Epoch 86/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9577 - accuracy: 0.2806 - val_loss: 1.9934 - val_accuracy: 0.2401\n",
      "Epoch 87/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9576 - accuracy: 0.2806 - val_loss: 1.9934 - val_accuracy: 0.2401\n",
      "Epoch 88/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9576 - accuracy: 0.2806 - val_loss: 1.9934 - val_accuracy: 0.2401\n",
      "Epoch 89/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9576 - accuracy: 0.2806 - val_loss: 1.9935 - val_accuracy: 0.2401\n",
      "Epoch 90/800\n",
      "1176/1176 [==============================] - 0s 17us/step - loss: 1.9575 - accuracy: 0.2806 - val_loss: 1.9936 - val_accuracy: 0.2401\n",
      "Epoch 91/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9575 - accuracy: 0.2806 - val_loss: 1.9938 - val_accuracy: 0.2401\n",
      "Epoch 92/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9574 - accuracy: 0.2806 - val_loss: 1.9940 - val_accuracy: 0.2401\n",
      "Epoch 93/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9574 - accuracy: 0.2806 - val_loss: 1.9940 - val_accuracy: 0.2401\n",
      "Epoch 94/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9574 - accuracy: 0.2806 - val_loss: 1.9940 - val_accuracy: 0.2401\n",
      "Epoch 95/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9573 - accuracy: 0.2806 - val_loss: 1.9940 - val_accuracy: 0.2401\n",
      "Epoch 96/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9572 - accuracy: 0.2806 - val_loss: 1.9940 - val_accuracy: 0.2401\n",
      "Epoch 97/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9572 - accuracy: 0.2806 - val_loss: 1.9940 - val_accuracy: 0.2401\n",
      "Epoch 98/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9574 - accuracy: 0.2806 - val_loss: 1.9941 - val_accuracy: 0.2401\n",
      "Epoch 99/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9574 - accuracy: 0.2806 - val_loss: 1.9944 - val_accuracy: 0.2401\n",
      "Epoch 100/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9571 - accuracy: 0.2806 - val_loss: 1.9946 - val_accuracy: 0.2401\n",
      "Epoch 101/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9571 - accuracy: 0.2806 - val_loss: 1.9947 - val_accuracy: 0.2401\n",
      "Epoch 102/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9570 - accuracy: 0.2806 - val_loss: 1.9947 - val_accuracy: 0.2401\n",
      "Epoch 103/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9570 - accuracy: 0.2806 - val_loss: 1.9946 - val_accuracy: 0.2401\n",
      "Epoch 104/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9570 - accuracy: 0.2806 - val_loss: 1.9942 - val_accuracy: 0.2401\n",
      "Epoch 105/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9568 - accuracy: 0.2806 - val_loss: 1.9944 - val_accuracy: 0.2401\n",
      "Epoch 106/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9568 - accuracy: 0.2806 - val_loss: 1.9946 - val_accuracy: 0.2401\n",
      "Epoch 107/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9568 - accuracy: 0.2806 - val_loss: 1.9948 - val_accuracy: 0.2401\n",
      "Epoch 108/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9568 - accuracy: 0.2806 - val_loss: 1.9949 - val_accuracy: 0.2401\n",
      "Epoch 109/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9567 - accuracy: 0.2806 - val_loss: 1.9949 - val_accuracy: 0.2401\n",
      "Epoch 110/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9567 - accuracy: 0.2806 - val_loss: 1.9950 - val_accuracy: 0.2401\n",
      "Epoch 111/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9569 - accuracy: 0.2806 - val_loss: 1.9949 - val_accuracy: 0.2401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9568 - accuracy: 0.2806 - val_loss: 1.9948 - val_accuracy: 0.2401\n",
      "Epoch 113/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9565 - accuracy: 0.2806 - val_loss: 1.9949 - val_accuracy: 0.2401\n",
      "Epoch 114/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9565 - accuracy: 0.2806 - val_loss: 1.9951 - val_accuracy: 0.2401\n",
      "Epoch 115/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9564 - accuracy: 0.2806 - val_loss: 1.9953 - val_accuracy: 0.2401\n",
      "Epoch 116/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9567 - accuracy: 0.2806 - val_loss: 1.9956 - val_accuracy: 0.2401\n",
      "Epoch 117/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9564 - accuracy: 0.2806 - val_loss: 1.9953 - val_accuracy: 0.2401\n",
      "Epoch 118/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9564 - accuracy: 0.2806 - val_loss: 1.9953 - val_accuracy: 0.2401\n",
      "Epoch 119/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9564 - accuracy: 0.2806 - val_loss: 1.9953 - val_accuracy: 0.2401\n",
      "Epoch 120/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9563 - accuracy: 0.2806 - val_loss: 1.9955 - val_accuracy: 0.2401\n",
      "Epoch 121/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9564 - accuracy: 0.2806 - val_loss: 1.9956 - val_accuracy: 0.2401\n",
      "Epoch 122/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9563 - accuracy: 0.2806 - val_loss: 1.9959 - val_accuracy: 0.2401\n",
      "Epoch 123/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9562 - accuracy: 0.2806 - val_loss: 1.9957 - val_accuracy: 0.2401\n",
      "Epoch 124/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9562 - accuracy: 0.2806 - val_loss: 1.9956 - val_accuracy: 0.2401\n",
      "Epoch 125/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9562 - accuracy: 0.2806 - val_loss: 1.9955 - val_accuracy: 0.2401\n",
      "Epoch 126/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9561 - accuracy: 0.2806 - val_loss: 1.9957 - val_accuracy: 0.2401\n",
      "Epoch 127/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9561 - accuracy: 0.2806 - val_loss: 1.9957 - val_accuracy: 0.2401\n",
      "Epoch 128/800\n",
      "1176/1176 [==============================] - 0s 10us/step - loss: 1.9563 - accuracy: 0.2781 - val_loss: 1.9956 - val_accuracy: 0.2361\n",
      "Epoch 129/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9560 - accuracy: 0.2772 - val_loss: 1.9957 - val_accuracy: 0.2361\n",
      "Epoch 130/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9561 - accuracy: 0.2806 - val_loss: 1.9961 - val_accuracy: 0.2401\n",
      "Epoch 131/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9560 - accuracy: 0.2806 - val_loss: 1.9962 - val_accuracy: 0.2401\n",
      "Epoch 132/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9560 - accuracy: 0.2806 - val_loss: 1.9960 - val_accuracy: 0.2401\n",
      "Epoch 133/800\n",
      "1176/1176 [==============================] - 0s 17us/step - loss: 1.9560 - accuracy: 0.2806 - val_loss: 1.9961 - val_accuracy: 0.2401\n",
      "Epoch 134/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9558 - accuracy: 0.2806 - val_loss: 1.9961 - val_accuracy: 0.2401\n",
      "Epoch 135/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9561 - accuracy: 0.2772 - val_loss: 1.9959 - val_accuracy: 0.2361\n",
      "Epoch 136/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9560 - accuracy: 0.2764 - val_loss: 1.9963 - val_accuracy: 0.2401\n",
      "Epoch 137/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9558 - accuracy: 0.2806 - val_loss: 1.9963 - val_accuracy: 0.2401\n",
      "Epoch 138/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9559 - accuracy: 0.2806 - val_loss: 1.9961 - val_accuracy: 0.2401\n",
      "Epoch 139/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9558 - accuracy: 0.2806 - val_loss: 1.9964 - val_accuracy: 0.2401\n",
      "Epoch 140/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9558 - accuracy: 0.2806 - val_loss: 1.9965 - val_accuracy: 0.2401\n",
      "Epoch 141/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9557 - accuracy: 0.2806 - val_loss: 1.9963 - val_accuracy: 0.2401\n",
      "Epoch 142/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9556 - accuracy: 0.2806 - val_loss: 1.9963 - val_accuracy: 0.2401\n",
      "Epoch 143/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9556 - accuracy: 0.2806 - val_loss: 1.9964 - val_accuracy: 0.2401\n",
      "Epoch 144/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9557 - accuracy: 0.2755 - val_loss: 1.9964 - val_accuracy: 0.2361\n",
      "Epoch 145/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9555 - accuracy: 0.2798 - val_loss: 1.9965 - val_accuracy: 0.2401\n",
      "Epoch 146/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9556 - accuracy: 0.2806 - val_loss: 1.9965 - val_accuracy: 0.2401\n",
      "Epoch 147/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9555 - accuracy: 0.2806 - val_loss: 1.9968 - val_accuracy: 0.2401\n",
      "Epoch 148/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9555 - accuracy: 0.2806 - val_loss: 1.9968 - val_accuracy: 0.2401\n",
      "Epoch 149/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9554 - accuracy: 0.2806 - val_loss: 1.9968 - val_accuracy: 0.2401\n",
      "Epoch 150/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9555 - accuracy: 0.2755 - val_loss: 1.9969 - val_accuracy: 0.2361\n",
      "Epoch 151/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9554 - accuracy: 0.2772 - val_loss: 1.9969 - val_accuracy: 0.2361\n",
      "Epoch 152/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9554 - accuracy: 0.2764 - val_loss: 1.9968 - val_accuracy: 0.2401\n",
      "Epoch 153/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9553 - accuracy: 0.2806 - val_loss: 1.9969 - val_accuracy: 0.2401\n",
      "Epoch 154/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9553 - accuracy: 0.2806 - val_loss: 1.9969 - val_accuracy: 0.2401\n",
      "Epoch 155/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9554 - accuracy: 0.2806 - val_loss: 1.9970 - val_accuracy: 0.2401\n",
      "Epoch 156/800\n",
      "1176/1176 [==============================] - 0s 15us/step - loss: 1.9553 - accuracy: 0.2806 - val_loss: 1.9969 - val_accuracy: 0.2401\n",
      "Epoch 157/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9553 - accuracy: 0.2806 - val_loss: 1.9968 - val_accuracy: 0.2401\n",
      "Epoch 158/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9552 - accuracy: 0.2806 - val_loss: 1.9970 - val_accuracy: 0.2361\n",
      "Epoch 159/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9552 - accuracy: 0.2772 - val_loss: 1.9970 - val_accuracy: 0.2361\n",
      "Epoch 160/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9552 - accuracy: 0.2772 - val_loss: 1.9971 - val_accuracy: 0.2361\n",
      "Epoch 161/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9551 - accuracy: 0.2772 - val_loss: 1.9971 - val_accuracy: 0.2361\n",
      "Epoch 162/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9552 - accuracy: 0.2772 - val_loss: 1.9971 - val_accuracy: 0.2361\n",
      "Epoch 163/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9551 - accuracy: 0.2772 - val_loss: 1.9972 - val_accuracy: 0.2361\n",
      "Epoch 164/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9551 - accuracy: 0.2772 - val_loss: 1.9974 - val_accuracy: 0.2361\n",
      "Epoch 165/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9551 - accuracy: 0.2798 - val_loss: 1.9974 - val_accuracy: 0.2401\n",
      "Epoch 166/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9551 - accuracy: 0.2798 - val_loss: 1.9974 - val_accuracy: 0.2361\n",
      "Epoch 167/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9550 - accuracy: 0.2772 - val_loss: 1.9973 - val_accuracy: 0.2361\n",
      "Epoch 168/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9550 - accuracy: 0.2772 - val_loss: 1.9974 - val_accuracy: 0.2361\n",
      "Epoch 169/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9550 - accuracy: 0.2772 - val_loss: 1.9974 - val_accuracy: 0.2361\n",
      "Epoch 170/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9551 - accuracy: 0.2772 - val_loss: 1.9974 - val_accuracy: 0.2361\n",
      "Epoch 171/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9548 - accuracy: 0.2772 - val_loss: 1.9974 - val_accuracy: 0.2361\n",
      "Epoch 172/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9550 - accuracy: 0.2781 - val_loss: 1.9974 - val_accuracy: 0.2361\n",
      "Epoch 173/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9549 - accuracy: 0.2772 - val_loss: 1.9976 - val_accuracy: 0.2401\n",
      "Epoch 174/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9549 - accuracy: 0.2806 - val_loss: 1.9978 - val_accuracy: 0.2401\n",
      "Epoch 175/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9548 - accuracy: 0.2781 - val_loss: 1.9976 - val_accuracy: 0.2361\n",
      "Epoch 176/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9548 - accuracy: 0.2772 - val_loss: 1.9976 - val_accuracy: 0.2361\n",
      "Epoch 177/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9548 - accuracy: 0.2798 - val_loss: 1.9978 - val_accuracy: 0.2361\n",
      "Epoch 178/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9548 - accuracy: 0.2772 - val_loss: 1.9977 - val_accuracy: 0.2361\n",
      "Epoch 179/800\n",
      "1176/1176 [==============================] - ETA: 0s - loss: 1.9624 - accuracy: 0.28 - 0s 12us/step - loss: 1.9547 - accuracy: 0.2772 - val_loss: 1.9976 - val_accuracy: 0.2361\n",
      "Epoch 180/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9547 - accuracy: 0.2772 - val_loss: 1.9975 - val_accuracy: 0.2361\n",
      "Epoch 181/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9547 - accuracy: 0.2772 - val_loss: 1.9977 - val_accuracy: 0.2361\n",
      "Epoch 182/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9546 - accuracy: 0.2772 - val_loss: 1.9978 - val_accuracy: 0.2361\n",
      "Epoch 183/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9546 - accuracy: 0.2772 - val_loss: 1.9980 - val_accuracy: 0.2361\n",
      "Epoch 184/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9549 - accuracy: 0.2815 - val_loss: 1.9982 - val_accuracy: 0.2401\n",
      "Epoch 185/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9546 - accuracy: 0.2806 - val_loss: 1.9981 - val_accuracy: 0.2401\n",
      "Epoch 186/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9546 - accuracy: 0.2823 - val_loss: 1.9979 - val_accuracy: 0.2361\n",
      "Epoch 187/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9546 - accuracy: 0.2772 - val_loss: 1.9978 - val_accuracy: 0.2361\n",
      "Epoch 188/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9546 - accuracy: 0.2772 - val_loss: 1.9979 - val_accuracy: 0.2361\n",
      "Epoch 189/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9546 - accuracy: 0.2772 - val_loss: 1.9981 - val_accuracy: 0.2361\n",
      "Epoch 190/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9545 - accuracy: 0.2772 - val_loss: 1.9981 - val_accuracy: 0.2361\n",
      "Epoch 191/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9545 - accuracy: 0.2772 - val_loss: 1.9981 - val_accuracy: 0.2361\n",
      "Epoch 192/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9545 - accuracy: 0.2772 - val_loss: 1.9981 - val_accuracy: 0.2361\n",
      "Epoch 193/800\n",
      "1176/1176 [==============================] - 0s 18us/step - loss: 1.9545 - accuracy: 0.2772 - val_loss: 1.9984 - val_accuracy: 0.2361\n",
      "Epoch 194/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9546 - accuracy: 0.2772 - val_loss: 1.9983 - val_accuracy: 0.2361\n",
      "Epoch 195/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9546 - accuracy: 0.2772 - val_loss: 1.9983 - val_accuracy: 0.2361\n",
      "Epoch 196/800\n",
      "1176/1176 [==============================] - 0s 15us/step - loss: 1.9544 - accuracy: 0.2772 - val_loss: 1.9985 - val_accuracy: 0.2361\n",
      "Epoch 197/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9544 - accuracy: 0.2772 - val_loss: 1.9986 - val_accuracy: 0.2361\n",
      "Epoch 198/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9545 - accuracy: 0.2772 - val_loss: 1.9983 - val_accuracy: 0.2361\n",
      "Epoch 199/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9546 - accuracy: 0.2772 - val_loss: 1.9984 - val_accuracy: 0.2361\n",
      "Epoch 200/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9544 - accuracy: 0.2772 - val_loss: 1.9985 - val_accuracy: 0.2361\n",
      "Epoch 201/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9544 - accuracy: 0.2772 - val_loss: 1.9983 - val_accuracy: 0.2361\n",
      "Epoch 202/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9544 - accuracy: 0.2772 - val_loss: 1.9983 - val_accuracy: 0.2361\n",
      "Epoch 203/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9543 - accuracy: 0.2772 - val_loss: 1.9985 - val_accuracy: 0.2361\n",
      "Epoch 204/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9543 - accuracy: 0.2772 - val_loss: 1.9986 - val_accuracy: 0.2361\n",
      "Epoch 205/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9543 - accuracy: 0.2772 - val_loss: 1.9987 - val_accuracy: 0.2361\n",
      "Epoch 206/800\n",
      "1176/1176 [==============================] - ETA: 0s - loss: 1.9532 - accuracy: 0.34 - 0s 12us/step - loss: 1.9542 - accuracy: 0.2772 - val_loss: 1.9986 - val_accuracy: 0.2361\n",
      "Epoch 207/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9542 - accuracy: 0.2772 - val_loss: 1.9987 - val_accuracy: 0.2361\n",
      "Epoch 208/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9541 - accuracy: 0.2772 - val_loss: 1.9989 - val_accuracy: 0.2361\n",
      "Epoch 209/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9541 - accuracy: 0.2772 - val_loss: 1.9989 - val_accuracy: 0.2361\n",
      "Epoch 210/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9542 - accuracy: 0.2772 - val_loss: 1.9988 - val_accuracy: 0.2361\n",
      "Epoch 211/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9541 - accuracy: 0.2772 - val_loss: 1.9989 - val_accuracy: 0.2361\n",
      "Epoch 212/800\n",
      "1176/1176 [==============================] - 0s 15us/step - loss: 1.9541 - accuracy: 0.2772 - val_loss: 1.9991 - val_accuracy: 0.2361\n",
      "Epoch 213/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9540 - accuracy: 0.2772 - val_loss: 1.9991 - val_accuracy: 0.2361\n",
      "Epoch 214/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9544 - accuracy: 0.2772 - val_loss: 1.9994 - val_accuracy: 0.2361\n",
      "Epoch 215/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9540 - accuracy: 0.2772 - val_loss: 1.9992 - val_accuracy: 0.2361\n",
      "Epoch 216/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9541 - accuracy: 0.2772 - val_loss: 1.9991 - val_accuracy: 0.2361\n",
      "Epoch 217/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9541 - accuracy: 0.2772 - val_loss: 1.9988 - val_accuracy: 0.2361\n",
      "Epoch 218/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9541 - accuracy: 0.2772 - val_loss: 1.9990 - val_accuracy: 0.2361\n",
      "Epoch 219/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9540 - accuracy: 0.2772 - val_loss: 1.9991 - val_accuracy: 0.2361\n",
      "Epoch 220/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9540 - accuracy: 0.2772 - val_loss: 1.9989 - val_accuracy: 0.2361\n",
      "Epoch 221/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9540 - accuracy: 0.2772 - val_loss: 1.9991 - val_accuracy: 0.2361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9540 - accuracy: 0.2772 - val_loss: 1.9992 - val_accuracy: 0.2361\n",
      "Epoch 223/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9539 - accuracy: 0.2772 - val_loss: 1.9991 - val_accuracy: 0.2361\n",
      "Epoch 224/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9540 - accuracy: 0.2772 - val_loss: 1.9993 - val_accuracy: 0.2361\n",
      "Epoch 225/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9539 - accuracy: 0.2772 - val_loss: 1.9994 - val_accuracy: 0.2361\n",
      "Epoch 226/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9539 - accuracy: 0.2772 - val_loss: 1.9994 - val_accuracy: 0.2361\n",
      "Epoch 227/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9538 - accuracy: 0.2772 - val_loss: 1.9993 - val_accuracy: 0.2361\n",
      "Epoch 228/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9539 - accuracy: 0.2772 - val_loss: 1.9994 - val_accuracy: 0.2361\n",
      "Epoch 229/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9539 - accuracy: 0.2772 - val_loss: 1.9993 - val_accuracy: 0.2361\n",
      "Epoch 230/800\n",
      "1176/1176 [==============================] - 0s 15us/step - loss: 1.9539 - accuracy: 0.2772 - val_loss: 1.9993 - val_accuracy: 0.2361\n",
      "Epoch 231/800\n",
      "1176/1176 [==============================] - 0s 25us/step - loss: 1.9538 - accuracy: 0.2772 - val_loss: 1.9993 - val_accuracy: 0.2361\n",
      "Epoch 232/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9539 - accuracy: 0.2772 - val_loss: 1.9995 - val_accuracy: 0.2361\n",
      "Epoch 233/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9538 - accuracy: 0.2772 - val_loss: 1.9995 - val_accuracy: 0.2361\n",
      "Epoch 234/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9539 - accuracy: 0.2772 - val_loss: 1.9995 - val_accuracy: 0.2361\n",
      "Epoch 235/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9538 - accuracy: 0.2772 - val_loss: 1.9995 - val_accuracy: 0.2361\n",
      "Epoch 236/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9538 - accuracy: 0.2772 - val_loss: 1.9998 - val_accuracy: 0.2361\n",
      "Epoch 237/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9537 - accuracy: 0.2772 - val_loss: 1.9997 - val_accuracy: 0.2361\n",
      "Epoch 238/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9537 - accuracy: 0.2772 - val_loss: 1.9998 - val_accuracy: 0.2361\n",
      "Epoch 239/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9536 - accuracy: 0.2772 - val_loss: 1.9998 - val_accuracy: 0.2361\n",
      "Epoch 240/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9537 - accuracy: 0.2772 - val_loss: 1.9999 - val_accuracy: 0.2361\n",
      "Epoch 241/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9537 - accuracy: 0.2772 - val_loss: 1.9999 - val_accuracy: 0.2361\n",
      "Epoch 242/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9537 - accuracy: 0.2772 - val_loss: 1.9998 - val_accuracy: 0.2361\n",
      "Epoch 243/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9537 - accuracy: 0.2772 - val_loss: 2.0000 - val_accuracy: 0.2361\n",
      "Epoch 244/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9536 - accuracy: 0.2772 - val_loss: 2.0001 - val_accuracy: 0.2361\n",
      "Epoch 245/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9537 - accuracy: 0.2772 - val_loss: 2.0002 - val_accuracy: 0.2361\n",
      "Epoch 246/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9535 - accuracy: 0.2772 - val_loss: 2.0001 - val_accuracy: 0.2361\n",
      "Epoch 247/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9537 - accuracy: 0.2772 - val_loss: 1.9999 - val_accuracy: 0.2361\n",
      "Epoch 248/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9536 - accuracy: 0.2772 - val_loss: 2.0000 - val_accuracy: 0.2361\n",
      "Epoch 249/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9536 - accuracy: 0.2772 - val_loss: 2.0000 - val_accuracy: 0.2361\n",
      "Epoch 250/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9536 - accuracy: 0.2772 - val_loss: 1.9999 - val_accuracy: 0.2361\n",
      "Epoch 251/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9536 - accuracy: 0.2772 - val_loss: 2.0002 - val_accuracy: 0.2361\n",
      "Epoch 252/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9535 - accuracy: 0.2772 - val_loss: 2.0002 - val_accuracy: 0.2361\n",
      "Epoch 253/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9535 - accuracy: 0.2772 - val_loss: 2.0001 - val_accuracy: 0.2361\n",
      "Epoch 254/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9535 - accuracy: 0.2772 - val_loss: 2.0001 - val_accuracy: 0.2361\n",
      "Epoch 255/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9534 - accuracy: 0.2772 - val_loss: 2.0002 - val_accuracy: 0.2361\n",
      "Epoch 256/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9535 - accuracy: 0.2772 - val_loss: 2.0003 - val_accuracy: 0.2361\n",
      "Epoch 257/800\n",
      "1176/1176 [==============================] - 0s 16us/step - loss: 1.9534 - accuracy: 0.2772 - val_loss: 2.0004 - val_accuracy: 0.2361\n",
      "Epoch 258/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9535 - accuracy: 0.2772 - val_loss: 2.0003 - val_accuracy: 0.2361\n",
      "Epoch 259/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9534 - accuracy: 0.2772 - val_loss: 2.0003 - val_accuracy: 0.2361\n",
      "Epoch 260/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9535 - accuracy: 0.2772 - val_loss: 2.0005 - val_accuracy: 0.2361\n",
      "Epoch 261/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9534 - accuracy: 0.2772 - val_loss: 2.0004 - val_accuracy: 0.2361\n",
      "Epoch 262/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9536 - accuracy: 0.2772 - val_loss: 2.0002 - val_accuracy: 0.2361\n",
      "Epoch 263/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9536 - accuracy: 0.2772 - val_loss: 2.0005 - val_accuracy: 0.2361\n",
      "Epoch 264/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9533 - accuracy: 0.2772 - val_loss: 2.0005 - val_accuracy: 0.2361\n",
      "Epoch 265/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9533 - accuracy: 0.2772 - val_loss: 2.0003 - val_accuracy: 0.2361\n",
      "Epoch 266/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9533 - accuracy: 0.2772 - val_loss: 2.0004 - val_accuracy: 0.2361\n",
      "Epoch 267/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9533 - accuracy: 0.2772 - val_loss: 2.0004 - val_accuracy: 0.2361\n",
      "Epoch 268/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9532 - accuracy: 0.2772 - val_loss: 2.0005 - val_accuracy: 0.2361\n",
      "Epoch 269/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9534 - accuracy: 0.2772 - val_loss: 2.0004 - val_accuracy: 0.2361\n",
      "Epoch 270/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9533 - accuracy: 0.2772 - val_loss: 2.0006 - val_accuracy: 0.2361\n",
      "Epoch 271/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9532 - accuracy: 0.2772 - val_loss: 2.0006 - val_accuracy: 0.2361\n",
      "Epoch 272/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9535 - accuracy: 0.2772 - val_loss: 2.0005 - val_accuracy: 0.2361\n",
      "Epoch 273/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9533 - accuracy: 0.2772 - val_loss: 2.0005 - val_accuracy: 0.2361\n",
      "Epoch 274/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9531 - accuracy: 0.2772 - val_loss: 2.0006 - val_accuracy: 0.2361\n",
      "Epoch 275/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9531 - accuracy: 0.2772 - val_loss: 2.0009 - val_accuracy: 0.2361\n",
      "Epoch 276/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9533 - accuracy: 0.2772 - val_loss: 2.0011 - val_accuracy: 0.2361\n",
      "Epoch 277/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9532 - accuracy: 0.2772 - val_loss: 2.0009 - val_accuracy: 0.2361\n",
      "Epoch 278/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9531 - accuracy: 0.2772 - val_loss: 2.0007 - val_accuracy: 0.2361\n",
      "Epoch 279/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9531 - accuracy: 0.2772 - val_loss: 2.0009 - val_accuracy: 0.2361\n",
      "Epoch 280/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9533 - accuracy: 0.2772 - val_loss: 2.0011 - val_accuracy: 0.2361\n",
      "Epoch 281/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9532 - accuracy: 0.2772 - val_loss: 2.0012 - val_accuracy: 0.2361\n",
      "Epoch 282/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9530 - accuracy: 0.2772 - val_loss: 2.0008 - val_accuracy: 0.2361\n",
      "Epoch 283/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9530 - accuracy: 0.2772 - val_loss: 2.0007 - val_accuracy: 0.2361\n",
      "Epoch 284/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9530 - accuracy: 0.2772 - val_loss: 2.0006 - val_accuracy: 0.2361\n",
      "Epoch 285/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9531 - accuracy: 0.2772 - val_loss: 2.0007 - val_accuracy: 0.2361\n",
      "Epoch 286/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9530 - accuracy: 0.2772 - val_loss: 2.0008 - val_accuracy: 0.2361\n",
      "Epoch 287/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0008 - val_accuracy: 0.2361\n",
      "Epoch 288/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9531 - accuracy: 0.2772 - val_loss: 2.0008 - val_accuracy: 0.2361\n",
      "Epoch 289/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9530 - accuracy: 0.2772 - val_loss: 2.0010 - val_accuracy: 0.2361\n",
      "Epoch 290/800\n",
      "1176/1176 [==============================] - 0s 10us/step - loss: 1.9530 - accuracy: 0.2772 - val_loss: 2.0012 - val_accuracy: 0.2361\n",
      "Epoch 291/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9530 - accuracy: 0.2772 - val_loss: 2.0012 - val_accuracy: 0.2361\n",
      "Epoch 292/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0010 - val_accuracy: 0.2361\n",
      "Epoch 293/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0012 - val_accuracy: 0.2361\n",
      "Epoch 294/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0012 - val_accuracy: 0.2361\n",
      "Epoch 295/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9530 - accuracy: 0.2772 - val_loss: 2.0014 - val_accuracy: 0.2361\n",
      "Epoch 296/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0013 - val_accuracy: 0.2361\n",
      "Epoch 297/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0014 - val_accuracy: 0.2361\n",
      "Epoch 298/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9528 - accuracy: 0.2772 - val_loss: 2.0014 - val_accuracy: 0.2361\n",
      "Epoch 299/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0015 - val_accuracy: 0.2361\n",
      "Epoch 300/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0015 - val_accuracy: 0.2361\n",
      "Epoch 301/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0014 - val_accuracy: 0.2361\n",
      "Epoch 302/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9528 - accuracy: 0.2772 - val_loss: 2.0013 - val_accuracy: 0.2361\n",
      "Epoch 303/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9528 - accuracy: 0.2772 - val_loss: 2.0013 - val_accuracy: 0.2361\n",
      "Epoch 304/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0015 - val_accuracy: 0.2361\n",
      "Epoch 305/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9528 - accuracy: 0.2772 - val_loss: 2.0014 - val_accuracy: 0.2361\n",
      "Epoch 306/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9528 - accuracy: 0.2772 - val_loss: 2.0015 - val_accuracy: 0.2361\n",
      "Epoch 307/800\n",
      "1176/1176 [==============================] - 0s 17us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0016 - val_accuracy: 0.2361\n",
      "Epoch 308/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9527 - accuracy: 0.2772 - val_loss: 2.0017 - val_accuracy: 0.2361\n",
      "Epoch 309/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9527 - accuracy: 0.2772 - val_loss: 2.0016 - val_accuracy: 0.2361\n",
      "Epoch 310/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9527 - accuracy: 0.2772 - val_loss: 2.0017 - val_accuracy: 0.2361\n",
      "Epoch 311/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9528 - accuracy: 0.2772 - val_loss: 2.0017 - val_accuracy: 0.2361\n",
      "Epoch 312/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9528 - accuracy: 0.2772 - val_loss: 2.0016 - val_accuracy: 0.2361\n",
      "Epoch 313/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9527 - accuracy: 0.2772 - val_loss: 2.0015 - val_accuracy: 0.2361\n",
      "Epoch 314/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9528 - accuracy: 0.2772 - val_loss: 2.0016 - val_accuracy: 0.2361\n",
      "Epoch 315/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9527 - accuracy: 0.2772 - val_loss: 2.0017 - val_accuracy: 0.2361\n",
      "Epoch 316/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9527 - accuracy: 0.2772 - val_loss: 2.0019 - val_accuracy: 0.2361\n",
      "Epoch 317/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9527 - accuracy: 0.2772 - val_loss: 2.0018 - val_accuracy: 0.2361\n",
      "Epoch 318/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9526 - accuracy: 0.2772 - val_loss: 2.0018 - val_accuracy: 0.2361\n",
      "Epoch 319/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9526 - accuracy: 0.2772 - val_loss: 2.0019 - val_accuracy: 0.2361\n",
      "Epoch 320/800\n",
      "1176/1176 [==============================] - 0s 16us/step - loss: 1.9526 - accuracy: 0.2772 - val_loss: 2.0018 - val_accuracy: 0.2361\n",
      "Epoch 321/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9526 - accuracy: 0.2772 - val_loss: 2.0018 - val_accuracy: 0.2361\n",
      "Epoch 322/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9527 - accuracy: 0.2772 - val_loss: 2.0020 - val_accuracy: 0.2361\n",
      "Epoch 323/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9526 - accuracy: 0.2772 - val_loss: 2.0017 - val_accuracy: 0.2361\n",
      "Epoch 324/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9525 - accuracy: 0.2772 - val_loss: 2.0019 - val_accuracy: 0.2361\n",
      "Epoch 325/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9525 - accuracy: 0.2772 - val_loss: 2.0020 - val_accuracy: 0.2361\n",
      "Epoch 326/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9525 - accuracy: 0.2772 - val_loss: 2.0022 - val_accuracy: 0.2361\n",
      "Epoch 327/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9525 - accuracy: 0.2772 - val_loss: 2.0021 - val_accuracy: 0.2361\n",
      "Epoch 328/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9529 - accuracy: 0.2772 - val_loss: 2.0018 - val_accuracy: 0.2361\n",
      "Epoch 329/800\n",
      "1176/1176 [==============================] - 0s 16us/step - loss: 1.9527 - accuracy: 0.2772 - val_loss: 2.0021 - val_accuracy: 0.2361\n",
      "Epoch 330/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9526 - accuracy: 0.2772 - val_loss: 2.0020 - val_accuracy: 0.2361\n",
      "Epoch 331/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9525 - accuracy: 0.2772 - val_loss: 2.0020 - val_accuracy: 0.2361\n",
      "Epoch 332/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9524 - accuracy: 0.2772 - val_loss: 2.0021 - val_accuracy: 0.2361\n",
      "Epoch 333/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9527 - accuracy: 0.2772 - val_loss: 2.0026 - val_accuracy: 0.2361\n",
      "Epoch 334/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9526 - accuracy: 0.2772 - val_loss: 2.0024 - val_accuracy: 0.2361\n",
      "Epoch 335/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9525 - accuracy: 0.2772 - val_loss: 2.0024 - val_accuracy: 0.2361\n",
      "Epoch 336/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9524 - accuracy: 0.2772 - val_loss: 2.0024 - val_accuracy: 0.2361\n",
      "Epoch 337/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9525 - accuracy: 0.2772 - val_loss: 2.0022 - val_accuracy: 0.2361\n",
      "Epoch 338/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0022 - val_accuracy: 0.2361\n",
      "Epoch 339/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9526 - accuracy: 0.2772 - val_loss: 2.0020 - val_accuracy: 0.2361\n",
      "Epoch 340/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9524 - accuracy: 0.2772 - val_loss: 2.0021 - val_accuracy: 0.2361\n",
      "Epoch 341/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9526 - accuracy: 0.2772 - val_loss: 2.0024 - val_accuracy: 0.2361\n",
      "Epoch 342/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9524 - accuracy: 0.2772 - val_loss: 2.0025 - val_accuracy: 0.2361\n",
      "Epoch 343/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9526 - accuracy: 0.2772 - val_loss: 2.0025 - val_accuracy: 0.2361\n",
      "Epoch 344/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9524 - accuracy: 0.2772 - val_loss: 2.0022 - val_accuracy: 0.2361\n",
      "Epoch 345/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0022 - val_accuracy: 0.2361\n",
      "Epoch 346/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0022 - val_accuracy: 0.2361\n",
      "Epoch 347/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9524 - accuracy: 0.2772 - val_loss: 2.0023 - val_accuracy: 0.2361\n",
      "Epoch 348/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9522 - accuracy: 0.2772 - val_loss: 2.0024 - val_accuracy: 0.2361\n",
      "Epoch 349/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0023 - val_accuracy: 0.2361\n",
      "Epoch 350/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0025 - val_accuracy: 0.2361\n",
      "Epoch 351/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0026 - val_accuracy: 0.2361\n",
      "Epoch 352/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9522 - accuracy: 0.2772 - val_loss: 2.0025 - val_accuracy: 0.2361\n",
      "Epoch 353/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0025 - val_accuracy: 0.2361\n",
      "Epoch 354/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0025 - val_accuracy: 0.2361\n",
      "Epoch 355/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0024 - val_accuracy: 0.2361\n",
      "Epoch 356/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0026 - val_accuracy: 0.2361\n",
      "Epoch 357/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9524 - accuracy: 0.2772 - val_loss: 2.0029 - val_accuracy: 0.2361\n",
      "Epoch 358/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9522 - accuracy: 0.2772 - val_loss: 2.0025 - val_accuracy: 0.2361\n",
      "Epoch 359/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9521 - accuracy: 0.2772 - val_loss: 2.0025 - val_accuracy: 0.2361\n",
      "Epoch 360/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0024 - val_accuracy: 0.2361\n",
      "Epoch 361/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9522 - accuracy: 0.2772 - val_loss: 2.0024 - val_accuracy: 0.2361\n",
      "Epoch 362/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0027 - val_accuracy: 0.2361\n",
      "Epoch 363/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9522 - accuracy: 0.2772 - val_loss: 2.0028 - val_accuracy: 0.2361\n",
      "Epoch 364/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9522 - accuracy: 0.2772 - val_loss: 2.0029 - val_accuracy: 0.2361\n",
      "Epoch 365/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9521 - accuracy: 0.2772 - val_loss: 2.0028 - val_accuracy: 0.2361\n",
      "Epoch 366/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9524 - accuracy: 0.2772 - val_loss: 2.0028 - val_accuracy: 0.2361\n",
      "Epoch 367/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9523 - accuracy: 0.2772 - val_loss: 2.0028 - val_accuracy: 0.2361\n",
      "Epoch 368/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9522 - accuracy: 0.2772 - val_loss: 2.0030 - val_accuracy: 0.2361\n",
      "Epoch 369/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9522 - accuracy: 0.2772 - val_loss: 2.0028 - val_accuracy: 0.2361\n",
      "Epoch 370/800\n",
      "1176/1176 [==============================] - 0s 10us/step - loss: 1.9521 - accuracy: 0.2772 - val_loss: 2.0029 - val_accuracy: 0.2361\n",
      "Epoch 371/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9522 - accuracy: 0.2772 - val_loss: 2.0029 - val_accuracy: 0.2361\n",
      "Epoch 372/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9520 - accuracy: 0.2772 - val_loss: 2.0030 - val_accuracy: 0.2361\n",
      "Epoch 373/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9520 - accuracy: 0.2772 - val_loss: 2.0030 - val_accuracy: 0.2361\n",
      "Epoch 374/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9521 - accuracy: 0.2772 - val_loss: 2.0028 - val_accuracy: 0.2361\n",
      "Epoch 375/800\n",
      "1176/1176 [==============================] - ETA: 0s - loss: 2.0274 - accuracy: 0.25 - 0s 12us/step - loss: 1.9521 - accuracy: 0.2772 - val_loss: 2.0029 - val_accuracy: 0.2361\n",
      "Epoch 376/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9521 - accuracy: 0.2772 - val_loss: 2.0031 - val_accuracy: 0.2361\n",
      "Epoch 377/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9520 - accuracy: 0.2772 - val_loss: 2.0031 - val_accuracy: 0.2361\n",
      "Epoch 378/800\n",
      "1176/1176 [==============================] - 0s 10us/step - loss: 1.9520 - accuracy: 0.2772 - val_loss: 2.0030 - val_accuracy: 0.2361\n",
      "Epoch 379/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9521 - accuracy: 0.2772 - val_loss: 2.0031 - val_accuracy: 0.2361\n",
      "Epoch 380/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9520 - accuracy: 0.2772 - val_loss: 2.0028 - val_accuracy: 0.2361\n",
      "Epoch 381/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9521 - accuracy: 0.2772 - val_loss: 2.0030 - val_accuracy: 0.2361\n",
      "Epoch 382/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9520 - accuracy: 0.2772 - val_loss: 2.0029 - val_accuracy: 0.2361\n",
      "Epoch 383/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9520 - accuracy: 0.2772 - val_loss: 2.0029 - val_accuracy: 0.2361\n",
      "Epoch 384/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9519 - accuracy: 0.2772 - val_loss: 2.0030 - val_accuracy: 0.2361\n",
      "Epoch 385/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9520 - accuracy: 0.2772 - val_loss: 2.0030 - val_accuracy: 0.2361\n",
      "Epoch 386/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9520 - accuracy: 0.2772 - val_loss: 2.0034 - val_accuracy: 0.2361\n",
      "Epoch 387/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9519 - accuracy: 0.2772 - val_loss: 2.0034 - val_accuracy: 0.2361\n",
      "Epoch 388/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9519 - accuracy: 0.2772 - val_loss: 2.0036 - val_accuracy: 0.2361\n",
      "Epoch 389/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9520 - accuracy: 0.2772 - val_loss: 2.0033 - val_accuracy: 0.2361\n",
      "Epoch 390/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0033 - val_accuracy: 0.2361\n",
      "Epoch 391/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9519 - accuracy: 0.2772 - val_loss: 2.0032 - val_accuracy: 0.2361\n",
      "Epoch 392/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9519 - accuracy: 0.2772 - val_loss: 2.0031 - val_accuracy: 0.2361\n",
      "Epoch 393/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9519 - accuracy: 0.2772 - val_loss: 2.0033 - val_accuracy: 0.2361\n",
      "Epoch 394/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9519 - accuracy: 0.2772 - val_loss: 2.0034 - val_accuracy: 0.2361\n",
      "Epoch 395/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9519 - accuracy: 0.2772 - val_loss: 2.0034 - val_accuracy: 0.2361\n",
      "Epoch 396/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0033 - val_accuracy: 0.2361\n",
      "Epoch 397/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0034 - val_accuracy: 0.2361\n",
      "Epoch 398/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9519 - accuracy: 0.2772 - val_loss: 2.0033 - val_accuracy: 0.2361\n",
      "Epoch 399/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0034 - val_accuracy: 0.2361\n",
      "Epoch 400/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0035 - val_accuracy: 0.2361\n",
      "Epoch 401/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0033 - val_accuracy: 0.2361\n",
      "Epoch 402/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0033 - val_accuracy: 0.2361\n",
      "Epoch 403/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0035 - val_accuracy: 0.2361\n",
      "Epoch 404/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9519 - accuracy: 0.2772 - val_loss: 2.0036 - val_accuracy: 0.2361\n",
      "Epoch 405/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0037 - val_accuracy: 0.2361\n",
      "Epoch 406/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9517 - accuracy: 0.2772 - val_loss: 2.0037 - val_accuracy: 0.2361\n",
      "Epoch 407/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0038 - val_accuracy: 0.2361\n",
      "Epoch 408/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9517 - accuracy: 0.2772 - val_loss: 2.0036 - val_accuracy: 0.2361\n",
      "Epoch 409/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0039 - val_accuracy: 0.2361\n",
      "Epoch 410/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9517 - accuracy: 0.2772 - val_loss: 2.0036 - val_accuracy: 0.2361\n",
      "Epoch 411/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9517 - accuracy: 0.2772 - val_loss: 2.0036 - val_accuracy: 0.2361\n",
      "Epoch 412/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9518 - accuracy: 0.2772 - val_loss: 2.0038 - val_accuracy: 0.2361\n",
      "Epoch 413/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9517 - accuracy: 0.2772 - val_loss: 2.0039 - val_accuracy: 0.2361\n",
      "Epoch 414/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9516 - accuracy: 0.2772 - val_loss: 2.0038 - val_accuracy: 0.2361\n",
      "Epoch 415/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9519 - accuracy: 0.2772 - val_loss: 2.0036 - val_accuracy: 0.2361\n",
      "Epoch 416/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9517 - accuracy: 0.2772 - val_loss: 2.0038 - val_accuracy: 0.2361\n",
      "Epoch 417/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9517 - accuracy: 0.2772 - val_loss: 2.0040 - val_accuracy: 0.2361\n",
      "Epoch 418/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9517 - accuracy: 0.2772 - val_loss: 2.0039 - val_accuracy: 0.2361\n",
      "Epoch 419/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9517 - accuracy: 0.2772 - val_loss: 2.0038 - val_accuracy: 0.2361\n",
      "Epoch 420/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9516 - accuracy: 0.2772 - val_loss: 2.0040 - val_accuracy: 0.2361\n",
      "Epoch 421/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9516 - accuracy: 0.2772 - val_loss: 2.0039 - val_accuracy: 0.2361\n",
      "Epoch 422/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9516 - accuracy: 0.2772 - val_loss: 2.0040 - val_accuracy: 0.2361\n",
      "Epoch 423/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9515 - accuracy: 0.2772 - val_loss: 2.0040 - val_accuracy: 0.2361\n",
      "Epoch 424/800\n",
      "1176/1176 [==============================] - 0s 21us/step - loss: 1.9517 - accuracy: 0.2772 - val_loss: 2.0041 - val_accuracy: 0.2361\n",
      "Epoch 425/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9516 - accuracy: 0.2772 - val_loss: 2.0039 - val_accuracy: 0.2361\n",
      "Epoch 426/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9516 - accuracy: 0.2772 - val_loss: 2.0040 - val_accuracy: 0.2361\n",
      "Epoch 427/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9516 - accuracy: 0.2772 - val_loss: 2.0040 - val_accuracy: 0.2361\n",
      "Epoch 428/800\n",
      "1176/1176 [==============================] - 0s 15us/step - loss: 1.9515 - accuracy: 0.2772 - val_loss: 2.0042 - val_accuracy: 0.2361\n",
      "Epoch 429/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9515 - accuracy: 0.2772 - val_loss: 2.0042 - val_accuracy: 0.2361\n",
      "Epoch 430/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9516 - accuracy: 0.2772 - val_loss: 2.0042 - val_accuracy: 0.2361\n",
      "Epoch 431/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9517 - accuracy: 0.2772 - val_loss: 2.0045 - val_accuracy: 0.2361\n",
      "Epoch 432/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9515 - accuracy: 0.2772 - val_loss: 2.0044 - val_accuracy: 0.2361\n",
      "Epoch 433/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9515 - accuracy: 0.2772 - val_loss: 2.0041 - val_accuracy: 0.2361\n",
      "Epoch 434/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9516 - accuracy: 0.2772 - val_loss: 2.0040 - val_accuracy: 0.2361\n",
      "Epoch 435/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9516 - accuracy: 0.2772 - val_loss: 2.0042 - val_accuracy: 0.2361\n",
      "Epoch 436/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9515 - accuracy: 0.2772 - val_loss: 2.0042 - val_accuracy: 0.2361\n",
      "Epoch 437/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9515 - accuracy: 0.2772 - val_loss: 2.0041 - val_accuracy: 0.2361\n",
      "Epoch 438/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0042 - val_accuracy: 0.2361\n",
      "Epoch 439/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9515 - accuracy: 0.2772 - val_loss: 2.0044 - val_accuracy: 0.2361\n",
      "Epoch 440/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0044 - val_accuracy: 0.2361\n",
      "Epoch 441/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9515 - accuracy: 0.2772 - val_loss: 2.0045 - val_accuracy: 0.2361\n",
      "Epoch 442/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 10us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0044 - val_accuracy: 0.2361\n",
      "Epoch 443/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0044 - val_accuracy: 0.2361\n",
      "Epoch 444/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9516 - accuracy: 0.2772 - val_loss: 2.0043 - val_accuracy: 0.2361\n",
      "Epoch 445/800\n",
      "1176/1176 [==============================] - 0s 20us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0044 - val_accuracy: 0.2361\n",
      "Epoch 446/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9515 - accuracy: 0.2772 - val_loss: 2.0044 - val_accuracy: 0.2361\n",
      "Epoch 447/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0045 - val_accuracy: 0.2361\n",
      "Epoch 448/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0045 - val_accuracy: 0.2361\n",
      "Epoch 449/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0047 - val_accuracy: 0.2361\n",
      "Epoch 450/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0045 - val_accuracy: 0.2361\n",
      "Epoch 451/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0043 - val_accuracy: 0.2361\n",
      "Epoch 452/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0042 - val_accuracy: 0.2361\n",
      "Epoch 453/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0043 - val_accuracy: 0.2361\n",
      "Epoch 454/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0046 - val_accuracy: 0.2361\n",
      "Epoch 455/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0046 - val_accuracy: 0.2361\n",
      "Epoch 456/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0045 - val_accuracy: 0.2361\n",
      "Epoch 457/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0044 - val_accuracy: 0.2361\n",
      "Epoch 458/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0045 - val_accuracy: 0.2361\n",
      "Epoch 459/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0046 - val_accuracy: 0.2361\n",
      "Epoch 460/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0046 - val_accuracy: 0.2361\n",
      "Epoch 461/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0045 - val_accuracy: 0.2361\n",
      "Epoch 462/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0045 - val_accuracy: 0.2361\n",
      "Epoch 463/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0047 - val_accuracy: 0.2361\n",
      "Epoch 464/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0047 - val_accuracy: 0.2361\n",
      "Epoch 465/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0047 - val_accuracy: 0.2361\n",
      "Epoch 466/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0046 - val_accuracy: 0.2361\n",
      "Epoch 467/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0048 - val_accuracy: 0.2361\n",
      "Epoch 468/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0047 - val_accuracy: 0.2361\n",
      "Epoch 469/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9514 - accuracy: 0.2772 - val_loss: 2.0047 - val_accuracy: 0.2361\n",
      "Epoch 470/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0047 - val_accuracy: 0.2361\n",
      "Epoch 471/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0048 - val_accuracy: 0.2361\n",
      "Epoch 472/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0046 - val_accuracy: 0.2361\n",
      "Epoch 473/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0046 - val_accuracy: 0.2361\n",
      "Epoch 474/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0046 - val_accuracy: 0.2361\n",
      "Epoch 475/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0048 - val_accuracy: 0.2361\n",
      "Epoch 476/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0049 - val_accuracy: 0.2361\n",
      "Epoch 477/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0050 - val_accuracy: 0.2361\n",
      "Epoch 478/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0049 - val_accuracy: 0.2361\n",
      "Epoch 479/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0047 - val_accuracy: 0.2361\n",
      "Epoch 480/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9511 - accuracy: 0.2772 - val_loss: 2.0047 - val_accuracy: 0.2361\n",
      "Epoch 481/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0050 - val_accuracy: 0.2361\n",
      "Epoch 482/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0047 - val_accuracy: 0.2361\n",
      "Epoch 483/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0046 - val_accuracy: 0.2361\n",
      "Epoch 484/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0049 - val_accuracy: 0.2361\n",
      "Epoch 485/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9511 - accuracy: 0.2772 - val_loss: 2.0049 - val_accuracy: 0.2361\n",
      "Epoch 486/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9511 - accuracy: 0.2772 - val_loss: 2.0050 - val_accuracy: 0.2361\n",
      "Epoch 487/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9511 - accuracy: 0.2772 - val_loss: 2.0051 - val_accuracy: 0.2361\n",
      "Epoch 488/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0051 - val_accuracy: 0.2361\n",
      "Epoch 489/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9511 - accuracy: 0.2772 - val_loss: 2.0050 - val_accuracy: 0.2361\n",
      "Epoch 490/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9511 - accuracy: 0.2772 - val_loss: 2.0051 - val_accuracy: 0.2361\n",
      "Epoch 491/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9511 - accuracy: 0.2772 - val_loss: 2.0052 - val_accuracy: 0.2361\n",
      "Epoch 492/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0052 - val_accuracy: 0.2361\n",
      "Epoch 493/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0051 - val_accuracy: 0.2361\n",
      "Epoch 494/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0050 - val_accuracy: 0.2361\n",
      "Epoch 495/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9511 - accuracy: 0.2772 - val_loss: 2.0051 - val_accuracy: 0.2361\n",
      "Epoch 496/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0054 - val_accuracy: 0.2361\n",
      "Epoch 497/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0052 - val_accuracy: 0.2361\n",
      "Epoch 498/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0051 - val_accuracy: 0.2361\n",
      "Epoch 499/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9512 - accuracy: 0.2772 - val_loss: 2.0054 - val_accuracy: 0.2361\n",
      "Epoch 500/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0053 - val_accuracy: 0.2361\n",
      "Epoch 501/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0052 - val_accuracy: 0.2361\n",
      "Epoch 502/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0052 - val_accuracy: 0.2361\n",
      "Epoch 503/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0053 - val_accuracy: 0.2361\n",
      "Epoch 504/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0053 - val_accuracy: 0.2361\n",
      "Epoch 505/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0053 - val_accuracy: 0.2361\n",
      "Epoch 506/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9513 - accuracy: 0.2772 - val_loss: 2.0052 - val_accuracy: 0.2361\n",
      "Epoch 507/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0052 - val_accuracy: 0.2361\n",
      "Epoch 508/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0054 - val_accuracy: 0.2361\n",
      "Epoch 509/800\n",
      "1176/1176 [==============================] - 0s 16us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0057 - val_accuracy: 0.2361\n",
      "Epoch 510/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0057 - val_accuracy: 0.2361\n",
      "Epoch 511/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0057 - val_accuracy: 0.2361\n",
      "Epoch 512/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0055 - val_accuracy: 0.2361\n",
      "Epoch 513/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0056 - val_accuracy: 0.2361\n",
      "Epoch 514/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0057 - val_accuracy: 0.2361\n",
      "Epoch 515/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0056 - val_accuracy: 0.2361\n",
      "Epoch 516/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0055 - val_accuracy: 0.2361\n",
      "Epoch 517/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0055 - val_accuracy: 0.2361\n",
      "Epoch 518/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9511 - accuracy: 0.2772 - val_loss: 2.0054 - val_accuracy: 0.2361\n",
      "Epoch 519/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0056 - val_accuracy: 0.2361\n",
      "Epoch 520/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0057 - val_accuracy: 0.2361\n",
      "Epoch 521/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0056 - val_accuracy: 0.2361\n",
      "Epoch 522/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0057 - val_accuracy: 0.2361\n",
      "Epoch 523/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0057 - val_accuracy: 0.2361\n",
      "Epoch 524/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9510 - accuracy: 0.2772 - val_loss: 2.0057 - val_accuracy: 0.2361\n",
      "Epoch 525/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0058 - val_accuracy: 0.2361\n",
      "Epoch 526/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0059 - val_accuracy: 0.2361\n",
      "Epoch 527/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0060 - val_accuracy: 0.2361\n",
      "Epoch 528/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0059 - val_accuracy: 0.2361\n",
      "Epoch 529/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0057 - val_accuracy: 0.2361\n",
      "Epoch 530/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0059 - val_accuracy: 0.2361\n",
      "Epoch 531/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0060 - val_accuracy: 0.2361\n",
      "Epoch 532/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0058 - val_accuracy: 0.2361\n",
      "Epoch 533/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0059 - val_accuracy: 0.2361\n",
      "Epoch 534/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0061 - val_accuracy: 0.2361\n",
      "Epoch 535/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9509 - accuracy: 0.2772 - val_loss: 2.0060 - val_accuracy: 0.2361\n",
      "Epoch 536/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0060 - val_accuracy: 0.2361\n",
      "Epoch 537/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0059 - val_accuracy: 0.2361\n",
      "Epoch 538/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0058 - val_accuracy: 0.2361\n",
      "Epoch 539/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0060 - val_accuracy: 0.2361\n",
      "Epoch 540/800\n",
      "1176/1176 [==============================] - 0s 22us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0061 - val_accuracy: 0.2361\n",
      "Epoch 541/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0062 - val_accuracy: 0.2361\n",
      "Epoch 542/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0063 - val_accuracy: 0.2361\n",
      "Epoch 543/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0060 - val_accuracy: 0.2361\n",
      "Epoch 544/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0059 - val_accuracy: 0.2361\n",
      "Epoch 545/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0060 - val_accuracy: 0.2361\n",
      "Epoch 546/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0060 - val_accuracy: 0.2361\n",
      "Epoch 547/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0061 - val_accuracy: 0.2361\n",
      "Epoch 548/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0061 - val_accuracy: 0.2361\n",
      "Epoch 549/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0063 - val_accuracy: 0.2361\n",
      "Epoch 550/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0065 - val_accuracy: 0.2361\n",
      "Epoch 551/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0064 - val_accuracy: 0.2361\n",
      "Epoch 552/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0064 - val_accuracy: 0.2361\n",
      "Epoch 553/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0063 - val_accuracy: 0.2361\n",
      "Epoch 554/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0062 - val_accuracy: 0.2361\n",
      "Epoch 555/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0061 - val_accuracy: 0.2361\n",
      "Epoch 556/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0061 - val_accuracy: 0.2361\n",
      "Epoch 557/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0062 - val_accuracy: 0.2361\n",
      "Epoch 558/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0063 - val_accuracy: 0.2361\n",
      "Epoch 559/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0063 - val_accuracy: 0.2361\n",
      "Epoch 560/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0065 - val_accuracy: 0.2361\n",
      "Epoch 561/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0061 - val_accuracy: 0.2361\n",
      "Epoch 562/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0062 - val_accuracy: 0.2361\n",
      "Epoch 563/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0063 - val_accuracy: 0.2361\n",
      "Epoch 564/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0065 - val_accuracy: 0.2361\n",
      "Epoch 565/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0066 - val_accuracy: 0.2361\n",
      "Epoch 566/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0065 - val_accuracy: 0.2361\n",
      "Epoch 567/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9508 - accuracy: 0.2772 - val_loss: 2.0067 - val_accuracy: 0.2361\n",
      "Epoch 568/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0066 - val_accuracy: 0.2361\n",
      "Epoch 569/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0067 - val_accuracy: 0.2361\n",
      "Epoch 570/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0065 - val_accuracy: 0.2361\n",
      "Epoch 571/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0063 - val_accuracy: 0.2361\n",
      "Epoch 572/800\n",
      "1176/1176 [==============================] - 0s 16us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0066 - val_accuracy: 0.2361\n",
      "Epoch 573/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0066 - val_accuracy: 0.2361\n",
      "Epoch 574/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0068 - val_accuracy: 0.2361\n",
      "Epoch 575/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 576/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0067 - val_accuracy: 0.2361\n",
      "Epoch 577/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0065 - val_accuracy: 0.2361\n",
      "Epoch 578/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0068 - val_accuracy: 0.2361\n",
      "Epoch 579/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 580/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0070 - val_accuracy: 0.2361\n",
      "Epoch 581/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 582/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0070 - val_accuracy: 0.2361\n",
      "Epoch 583/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9507 - accuracy: 0.2772 - val_loss: 2.0067 - val_accuracy: 0.2361\n",
      "Epoch 584/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0068 - val_accuracy: 0.2361\n",
      "Epoch 585/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 586/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0068 - val_accuracy: 0.2361\n",
      "Epoch 587/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 588/800\n",
      "1176/1176 [==============================] - ETA: 0s - loss: 1.9111 - accuracy: 0.28 - 0s 12us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 589/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0068 - val_accuracy: 0.2361\n",
      "Epoch 590/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0070 - val_accuracy: 0.2361\n",
      "Epoch 591/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 592/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 593/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0070 - val_accuracy: 0.2361\n",
      "Epoch 594/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 595/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9506 - accuracy: 0.2772 - val_loss: 2.0067 - val_accuracy: 0.2361\n",
      "Epoch 596/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0070 - val_accuracy: 0.2361\n",
      "Epoch 597/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0070 - val_accuracy: 0.2361\n",
      "Epoch 598/800\n",
      "1176/1176 [==============================] - 0s 23us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0071 - val_accuracy: 0.2361\n",
      "Epoch 599/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 600/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 601/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0068 - val_accuracy: 0.2361\n",
      "Epoch 602/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 603/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0069 - val_accuracy: 0.2361\n",
      "Epoch 604/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0072 - val_accuracy: 0.2361\n",
      "Epoch 605/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0074 - val_accuracy: 0.2361\n",
      "Epoch 606/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0072 - val_accuracy: 0.2361\n",
      "Epoch 607/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 608/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0074 - val_accuracy: 0.2361\n",
      "Epoch 609/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0074 - val_accuracy: 0.2361\n",
      "Epoch 610/800\n",
      "1176/1176 [==============================] - 0s 10us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 611/800\n",
      "1176/1176 [==============================] - 0s 10us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0071 - val_accuracy: 0.2361\n",
      "Epoch 612/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 613/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0074 - val_accuracy: 0.2361\n",
      "Epoch 614/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 615/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0071 - val_accuracy: 0.2361\n",
      "Epoch 616/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 617/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 618/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 619/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0074 - val_accuracy: 0.2361\n",
      "Epoch 620/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 621/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 622/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0076 - val_accuracy: 0.2361\n",
      "Epoch 623/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 624/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0074 - val_accuracy: 0.2361\n",
      "Epoch 625/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9504 - accuracy: 0.2772 - val_loss: 2.0071 - val_accuracy: 0.2361\n",
      "Epoch 626/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 627/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0075 - val_accuracy: 0.2361\n",
      "Epoch 628/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0074 - val_accuracy: 0.2361\n",
      "Epoch 629/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0073 - val_accuracy: 0.2361\n",
      "Epoch 630/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0076 - val_accuracy: 0.2361\n",
      "Epoch 631/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0076 - val_accuracy: 0.2361\n",
      "Epoch 632/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9505 - accuracy: 0.2772 - val_loss: 2.0072 - val_accuracy: 0.2361\n",
      "Epoch 633/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0075 - val_accuracy: 0.2361\n",
      "Epoch 634/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0076 - val_accuracy: 0.2361\n",
      "Epoch 635/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0075 - val_accuracy: 0.2361\n",
      "Epoch 636/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0076 - val_accuracy: 0.2361\n",
      "Epoch 637/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0075 - val_accuracy: 0.2361\n",
      "Epoch 638/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0076 - val_accuracy: 0.2361\n",
      "Epoch 639/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0076 - val_accuracy: 0.2361\n",
      "Epoch 640/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0077 - val_accuracy: 0.2361\n",
      "Epoch 641/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0079 - val_accuracy: 0.2361\n",
      "Epoch 642/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0079 - val_accuracy: 0.2361\n",
      "Epoch 643/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0079 - val_accuracy: 0.2361\n",
      "Epoch 644/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0079 - val_accuracy: 0.2361\n",
      "Epoch 645/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0078 - val_accuracy: 0.2361\n",
      "Epoch 646/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0076 - val_accuracy: 0.2361\n",
      "Epoch 647/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0078 - val_accuracy: 0.2361\n",
      "Epoch 648/800\n",
      "1176/1176 [==============================] - 0s 19us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0079 - val_accuracy: 0.2361\n",
      "Epoch 649/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0076 - val_accuracy: 0.2361\n",
      "Epoch 650/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0076 - val_accuracy: 0.2361\n",
      "Epoch 651/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0077 - val_accuracy: 0.2361\n",
      "Epoch 652/800\n",
      "1176/1176 [==============================] - ETA: 0s - loss: 1.9831 - accuracy: 0.29 - 0s 12us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0076 - val_accuracy: 0.2361\n",
      "Epoch 653/800\n",
      "1176/1176 [==============================] - 0s 16us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0078 - val_accuracy: 0.2361\n",
      "Epoch 654/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0079 - val_accuracy: 0.2361\n",
      "Epoch 655/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0081 - val_accuracy: 0.2361\n",
      "Epoch 656/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0079 - val_accuracy: 0.2361\n",
      "Epoch 657/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0079 - val_accuracy: 0.2361\n",
      "Epoch 658/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0080 - val_accuracy: 0.2361\n",
      "Epoch 659/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0079 - val_accuracy: 0.2361\n",
      "Epoch 660/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0078 - val_accuracy: 0.2361\n",
      "Epoch 661/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9503 - accuracy: 0.2772 - val_loss: 2.0077 - val_accuracy: 0.2361\n",
      "Epoch 662/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0078 - val_accuracy: 0.2361\n",
      "Epoch 663/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0080 - val_accuracy: 0.2361\n",
      "Epoch 664/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0080 - val_accuracy: 0.2361\n",
      "Epoch 665/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9502 - accuracy: 0.2772 - val_loss: 2.0079 - val_accuracy: 0.2361\n",
      "Epoch 666/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0081 - val_accuracy: 0.2361\n",
      "Epoch 667/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0081 - val_accuracy: 0.2361\n",
      "Epoch 668/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0081 - val_accuracy: 0.2361\n",
      "Epoch 669/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0083 - val_accuracy: 0.2361\n",
      "Epoch 670/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0082 - val_accuracy: 0.2361\n",
      "Epoch 671/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0082 - val_accuracy: 0.2361\n",
      "Epoch 672/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0081 - val_accuracy: 0.2361\n",
      "Epoch 673/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0084 - val_accuracy: 0.2361\n",
      "Epoch 674/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0082 - val_accuracy: 0.2361\n",
      "Epoch 675/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0081 - val_accuracy: 0.2361\n",
      "Epoch 676/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0081 - val_accuracy: 0.2361\n",
      "Epoch 677/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0081 - val_accuracy: 0.2361\n",
      "Epoch 678/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0083 - val_accuracy: 0.2361\n",
      "Epoch 679/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0084 - val_accuracy: 0.2361\n",
      "Epoch 680/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0085 - val_accuracy: 0.2361\n",
      "Epoch 681/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0085 - val_accuracy: 0.2361\n",
      "Epoch 682/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0083 - val_accuracy: 0.2361\n",
      "Epoch 683/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9501 - accuracy: 0.2772 - val_loss: 2.0083 - val_accuracy: 0.2361\n",
      "Epoch 684/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0082 - val_accuracy: 0.2361\n",
      "Epoch 685/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0082 - val_accuracy: 0.2361\n",
      "Epoch 686/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0083 - val_accuracy: 0.2361\n",
      "Epoch 687/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0084 - val_accuracy: 0.2361\n",
      "Epoch 688/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0086 - val_accuracy: 0.2361\n",
      "Epoch 689/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0085 - val_accuracy: 0.2361\n",
      "Epoch 690/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0084 - val_accuracy: 0.2361\n",
      "Epoch 691/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0083 - val_accuracy: 0.2361\n",
      "Epoch 692/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0085 - val_accuracy: 0.2361\n",
      "Epoch 693/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0088 - val_accuracy: 0.2361\n",
      "Epoch 694/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0086 - val_accuracy: 0.2361\n",
      "Epoch 695/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0087 - val_accuracy: 0.2361\n",
      "Epoch 696/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0085 - val_accuracy: 0.2361\n",
      "Epoch 697/800\n",
      "1176/1176 [==============================] - 0s 16us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0085 - val_accuracy: 0.2361\n",
      "Epoch 698/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0087 - val_accuracy: 0.2361\n",
      "Epoch 699/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0086 - val_accuracy: 0.2361\n",
      "Epoch 700/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0086 - val_accuracy: 0.2361\n",
      "Epoch 701/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0087 - val_accuracy: 0.2361\n",
      "Epoch 702/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0086 - val_accuracy: 0.2361\n",
      "Epoch 703/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0085 - val_accuracy: 0.2361\n",
      "Epoch 704/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0087 - val_accuracy: 0.2361\n",
      "Epoch 705/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0084 - val_accuracy: 0.2361\n",
      "Epoch 706/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0084 - val_accuracy: 0.2361\n",
      "Epoch 707/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0084 - val_accuracy: 0.2361\n",
      "Epoch 708/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0085 - val_accuracy: 0.2361\n",
      "Epoch 709/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0085 - val_accuracy: 0.2361\n",
      "Epoch 710/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0086 - val_accuracy: 0.2361\n",
      "Epoch 711/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0088 - val_accuracy: 0.2361\n",
      "Epoch 712/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0089 - val_accuracy: 0.2361\n",
      "Epoch 713/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0088 - val_accuracy: 0.2361\n",
      "Epoch 714/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9500 - accuracy: 0.2772 - val_loss: 2.0089 - val_accuracy: 0.2361\n",
      "Epoch 715/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0088 - val_accuracy: 0.2361\n",
      "Epoch 716/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0090 - val_accuracy: 0.2361\n",
      "Epoch 717/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0090 - val_accuracy: 0.2361\n",
      "Epoch 718/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0091 - val_accuracy: 0.2361\n",
      "Epoch 719/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0089 - val_accuracy: 0.2361\n",
      "Epoch 720/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0089 - val_accuracy: 0.2361\n",
      "Epoch 721/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0089 - val_accuracy: 0.2361\n",
      "Epoch 722/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0089 - val_accuracy: 0.2361\n",
      "Epoch 723/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0088 - val_accuracy: 0.2361\n",
      "Epoch 724/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0089 - val_accuracy: 0.2361\n",
      "Epoch 725/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0088 - val_accuracy: 0.2361\n",
      "Epoch 726/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0089 - val_accuracy: 0.2361\n",
      "Epoch 727/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0091 - val_accuracy: 0.2361\n",
      "Epoch 728/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0092 - val_accuracy: 0.2361\n",
      "Epoch 729/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0091 - val_accuracy: 0.2361\n",
      "Epoch 730/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0091 - val_accuracy: 0.2361\n",
      "Epoch 731/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0092 - val_accuracy: 0.2361\n",
      "Epoch 732/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0089 - val_accuracy: 0.2361\n",
      "Epoch 733/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0089 - val_accuracy: 0.2361\n",
      "Epoch 734/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0090 - val_accuracy: 0.2361\n",
      "Epoch 735/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0092 - val_accuracy: 0.2361\n",
      "Epoch 736/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0090 - val_accuracy: 0.2361\n",
      "Epoch 737/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0088 - val_accuracy: 0.2361\n",
      "Epoch 738/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0092 - val_accuracy: 0.2361\n",
      "Epoch 739/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0092 - val_accuracy: 0.2361\n",
      "Epoch 740/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0091 - val_accuracy: 0.2361\n",
      "Epoch 741/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0093 - val_accuracy: 0.2361\n",
      "Epoch 742/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0092 - val_accuracy: 0.2361\n",
      "Epoch 743/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0091 - val_accuracy: 0.2361\n",
      "Epoch 744/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0090 - val_accuracy: 0.2361\n",
      "Epoch 745/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0090 - val_accuracy: 0.2361\n",
      "Epoch 746/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9498 - accuracy: 0.2772 - val_loss: 2.0091 - val_accuracy: 0.2361\n",
      "Epoch 747/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0094 - val_accuracy: 0.2361\n",
      "Epoch 748/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9499 - accuracy: 0.2772 - val_loss: 2.0098 - val_accuracy: 0.2361\n",
      "Epoch 749/800\n",
      "1176/1176 [==============================] - 0s 10us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0098 - val_accuracy: 0.2361\n",
      "Epoch 750/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0097 - val_accuracy: 0.2361\n",
      "Epoch 751/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0095 - val_accuracy: 0.2361\n",
      "Epoch 752/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0096 - val_accuracy: 0.2361\n",
      "Epoch 753/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0096 - val_accuracy: 0.2361\n",
      "Epoch 754/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0096 - val_accuracy: 0.2361\n",
      "Epoch 755/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0095 - val_accuracy: 0.2361\n",
      "Epoch 756/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0096 - val_accuracy: 0.2361\n",
      "Epoch 757/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0096 - val_accuracy: 0.2361\n",
      "Epoch 758/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0094 - val_accuracy: 0.2361\n",
      "Epoch 759/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0095 - val_accuracy: 0.2361\n",
      "Epoch 760/800\n",
      "1176/1176 [==============================] - 0s 15us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0093 - val_accuracy: 0.2361\n",
      "Epoch 761/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0096 - val_accuracy: 0.2361\n",
      "Epoch 762/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0095 - val_accuracy: 0.2361\n",
      "Epoch 763/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0095 - val_accuracy: 0.2361\n",
      "Epoch 764/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0094 - val_accuracy: 0.2361\n",
      "Epoch 765/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0095 - val_accuracy: 0.2361\n",
      "Epoch 766/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0094 - val_accuracy: 0.2361\n",
      "Epoch 767/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0095 - val_accuracy: 0.2361\n",
      "Epoch 768/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0095 - val_accuracy: 0.2361\n",
      "Epoch 769/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0094 - val_accuracy: 0.2361\n",
      "Epoch 770/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0096 - val_accuracy: 0.2361\n",
      "Epoch 771/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0098 - val_accuracy: 0.2361\n",
      "Epoch 772/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0099 - val_accuracy: 0.2361\n",
      "Epoch 773/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0097 - val_accuracy: 0.2361\n",
      "Epoch 774/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0100 - val_accuracy: 0.2361\n",
      "Epoch 775/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0099 - val_accuracy: 0.2361\n",
      "Epoch 776/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0098 - val_accuracy: 0.2361\n",
      "Epoch 777/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0097 - val_accuracy: 0.2361\n",
      "Epoch 778/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0099 - val_accuracy: 0.2361\n",
      "Epoch 779/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0098 - val_accuracy: 0.2361\n",
      "Epoch 780/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9496 - accuracy: 0.2772 - val_loss: 2.0101 - val_accuracy: 0.2361\n",
      "Epoch 781/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0101 - val_accuracy: 0.2361\n",
      "Epoch 782/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0101 - val_accuracy: 0.2361\n",
      "Epoch 783/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0099 - val_accuracy: 0.2361\n",
      "Epoch 784/800\n",
      "1176/1176 [==============================] - 0s 14us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0101 - val_accuracy: 0.2361\n",
      "Epoch 785/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0101 - val_accuracy: 0.2361\n",
      "Epoch 786/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9494 - accuracy: 0.2772 - val_loss: 2.0100 - val_accuracy: 0.2361\n",
      "Epoch 787/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9494 - accuracy: 0.2772 - val_loss: 2.0100 - val_accuracy: 0.2361\n",
      "Epoch 788/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0099 - val_accuracy: 0.2361\n",
      "Epoch 789/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9494 - accuracy: 0.2772 - val_loss: 2.0101 - val_accuracy: 0.2361\n",
      "Epoch 790/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0102 - val_accuracy: 0.2361\n",
      "Epoch 791/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9494 - accuracy: 0.2772 - val_loss: 2.0103 - val_accuracy: 0.2361\n",
      "Epoch 792/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0105 - val_accuracy: 0.2361\n",
      "Epoch 793/800\n",
      "1176/1176 [==============================] - 0s 13us/step - loss: 1.9493 - accuracy: 0.2772 - val_loss: 2.0103 - val_accuracy: 0.2361\n",
      "Epoch 794/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9494 - accuracy: 0.2772 - val_loss: 2.0101 - val_accuracy: 0.2361\n",
      "Epoch 795/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9494 - accuracy: 0.2772 - val_loss: 2.0100 - val_accuracy: 0.2361\n",
      "Epoch 796/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0099 - val_accuracy: 0.2361\n",
      "Epoch 797/800\n",
      "1176/1176 [==============================] - 0s 11us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0097 - val_accuracy: 0.2361\n",
      "Epoch 798/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9494 - accuracy: 0.2772 - val_loss: 2.0099 - val_accuracy: 0.2361\n",
      "Epoch 799/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9495 - accuracy: 0.2772 - val_loss: 2.0099 - val_accuracy: 0.2361\n",
      "Epoch 800/800\n",
      "1176/1176 [==============================] - 0s 12us/step - loss: 1.9497 - accuracy: 0.2772 - val_loss: 2.0103 - val_accuracy: 0.2361\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(train_data,train_targets,\n",
    "                   epochs=800,\n",
    "                   batch_size=200,\n",
    "                   validation_data=(test_data,test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 35347 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 32244 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 33287 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 39511 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 35657 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30340 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25613 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 22833 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20989 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25976 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 35347 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 32244 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 33287 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 39511 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 35657 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30340 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25613 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 22833 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20989 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25976 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#繪圖\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss)+ 1)\n",
    "plt.plot(epochs, loss,'bo',label='Training loss')\n",
    "plt.plot(epochs, val_loss,'b',label='Validation loss')\n",
    "plt.title('訓練與驗證的損失函數')\n",
    "plt.xlabel('Epohs')\n",
    "plt.xlabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc = history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo',label='Training acc')\n",
    "plt.plot(epochs, val_acc,'b',label='Validation acc')\n",
    "plt.title('TAT')\n",
    "plt.xlabel('Epohs')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試 小綠同學"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>serveTime</th>\n",
       "      <th>Loan</th>\n",
       "      <th>SalPerY</th>\n",
       "      <th>holdCard</th>\n",
       "      <th>Career</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>120</td>\n",
       "      <td>4</td>\n",
       "      <td>600000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age serveTime Loan SalPerY holdCard Career\n",
       "0   8       120    4  600000        1      1\n",
       "1  28        12    0  600000        0      0\n",
       "2  28        12    0      87        2      0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "小綠 = pd.DataFrame(columns=[\"age\",\"serveTime\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\"])\n",
    "小綠.loc[0]=8,120,4,600000,1,1\n",
    "小綠.loc[1]=28,12,0,600000,0,0\n",
    "小綠.loc[2]=28,12,0,87,2,0\n",
    "小綠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#轉array\n",
    "小綠 = np.array(小綠).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1176, 6)\n",
      "(504, 6)\n",
      "(1176, 1)\n",
      "(504, 1)\n"
     ]
    }
   ],
   "source": [
    "#先打散資料(三次)\n",
    "for i in range(3):\n",
    "    df = shuffle(df)\n",
    "#再切成訓練與測試\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(df.loc[:, [\"age\",\"serveTime\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\"]] , df.loc[:, [\"credLimit_group\"]] , test_size=0.3, random_state=42)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_targets.shape)\n",
    "print(test_targets.shape)\n",
    "#轉array\n",
    "train_data = np.array(train_data).astype(float)\n",
    "test_data = np.array(test_data).astype(float)\n",
    "train_targets = np.array(train_targets).astype(int)\n",
    "test_targets = np.array(test_targets).astype(int)\n",
    "#把Y弄成onehot\n",
    "def to_one_hot(labels, dimension=20):\n",
    "    results = np.zeros((len(labels),dimension))\n",
    "    for i,label in enumerate(labels):\n",
    "        results[i,label]=1.\n",
    "    return results\n",
    "train_targets = to_one_hot(train_targets)\n",
    "test_targets = to_one_hot(test_targets)\n",
    "train_data_max = train_data.max(axis=0)\n",
    "train_data_min = train_data.min(axis=0)\n",
    "train_data_range = train_data_max-train_data_min\n",
    "小綠-=train_data_min\n",
    "小綠/=train_data_range\n",
    "#多一維\n",
    "小綠 = np.array(小綠).reshape((小綠.shape[0], 小綠.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_13_input to have 2 dimensions, but got array with shape (3, 6, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-3b88745f2a22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 他給出的是每一群的機率(相加為一)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m小綠\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"一號小綠被分在第\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"群\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m小綠\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected embedding_13_input to have 2 dimensions, but got array with shape (3, 6, 1)"
     ]
    }
   ],
   "source": [
    "# 他給出的是每一群的機率(相加為一)\n",
    "preds = model.predict(小綠)\n",
    "[print(\"一號小綠被分在第\",preds[i],\"群\") for i in range(len(小綠))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
