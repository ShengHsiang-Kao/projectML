{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "from keras.datasets import reuters\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_validate\n",
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1680 entries, 0 to 1686\n",
      "Data columns (total 8 columns):\n",
      "age                1680 non-null float64\n",
      "serveTime          1680 non-null float64\n",
      "credLimit          1680 non-null int64\n",
      "Loan               1680 non-null float64\n",
      "SalPerY            1680 non-null int64\n",
      "holdCard           1680 non-null int64\n",
      "Career             1680 non-null int64\n",
      "credLimit_group    1680 non-null int32\n",
      "dtypes: float64(3), int32(1), int64(4)\n",
      "memory usage: 111.6 KB\n"
     ]
    }
   ],
   "source": [
    "#開檔\n",
    "df = pd.read_excel(r'C:\\Users\\Big data\\Desktop\\class\\funcardproject\\斷詞與和卡額度_20群.xls',encoding='utf-16')\n",
    "df = df.loc[:, [\"age\",\"serveTime\",\"credLimit\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\",\"credLimit_group\"]] \n",
    "#若某raw有NAN則整RAW刪除\n",
    "df =df.dropna(\n",
    "    axis=0,     # 0: 对行进行操作; 1: 对列进行操作\n",
    "    how='any'   # 'any': 只要存在 NaN 就 drop 掉; 'all': 必须全部是 NaN 才 drop \n",
    "    ) \n",
    "#把分群的Y轉成int\n",
    "df['credLimit_group'] = df['credLimit_group'].astype('int')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1176, 6)\n",
      "(504, 6)\n",
      "(1176, 1)\n",
      "(504, 1)\n"
     ]
    }
   ],
   "source": [
    "#先打散資料(三次)\n",
    "for i in range(3):\n",
    "    df = shuffle(df)\n",
    "#再切成訓練與測試\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(df.loc[:, [\"age\",\"serveTime\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\"]] , df.loc[:, [\"credLimit_group\"]] , test_size=0.3, random_state=4)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_targets.shape)\n",
    "print(test_targets.shape)\n",
    "#轉array\n",
    "train_data = np.array(train_data).astype(float)\n",
    "test_data = np.array(test_data).astype(float)\n",
    "train_targets = np.array(train_targets).astype(int)\n",
    "test_targets = np.array(test_targets).astype(int)\n",
    "#把Y弄成onehot\n",
    "def to_one_hot(labels, dimension=20):\n",
    "    results = np.zeros((len(labels),dimension))\n",
    "    for i,label in enumerate(labels):\n",
    "        results[i,label]=1.\n",
    "    return results\n",
    "train_targets = to_one_hot(train_targets)\n",
    "test_targets = to_one_hot(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1176, 6)\n",
      "(504, 6)\n",
      "(1176, 20)\n",
      "(504, 20)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_targets.shape)\n",
    "print(test_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正規化(例如本資料serveTime與SalPerY不同單位且數值差異甚大,因此需轉為標準差)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 標準化\n",
    "# mean = train_data.mean(axis=0)\n",
    "# train_data -=mean\n",
    "# std = train_data.std(axis=0)\n",
    "# train_data/=std\n",
    "# test_data-=mean\n",
    "# test_data/=std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正規化\n",
    "#因為relu,所以這個比較好\n",
    "train_data_max = train_data.max(axis=0)\n",
    "train_data_min = train_data.min(axis=0)\n",
    "train_data_range = train_data_max-train_data_min\n",
    "train_data-=train_data_min\n",
    "train_data/=train_data_range\n",
    "\n",
    "test_data_max = test_data.max(axis=0)\n",
    "test_data_min = test_data.min(axis=0)\n",
    "test_data_range = test_data_max-test_data_min\n",
    "test_data-=test_data_min\n",
    "test_data/=test_data_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train_data).reshape((train_data.shape[0], train_data.shape[1], 1))\n",
    "X_test = np.array(test_data).reshape((test_data.shape[0], test_data.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(30, input_shape = (6,1), return_sequences = True))\n",
    "    model.add(LSTM(30, return_sequences = True))\n",
    "    model.add(LSTM(30, return_sequences = False))\n",
    "    model.add(Dense(20))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    Nadam = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=0.01, schedule_decay=0.004)\n",
    "    RMSprop = keras.optimizers.RMSprop(lr=0.001, rho=0.09, epsilon=0.001, decay=0.001)\n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = RMSprop, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1176 samples, validate on 504 samples\n",
      "Epoch 1/700\n",
      "1176/1176 [==============================] - 1s 789us/step - loss: 2.9887 - accuracy: 0.2032 - val_loss: 2.9783 - val_accuracy: 0.2619\n",
      "Epoch 2/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.9706 - accuracy: 0.2696 - val_loss: 2.9586 - val_accuracy: 0.2619\n",
      "Epoch 3/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 2.9495 - accuracy: 0.2696 - val_loss: 2.9351 - val_accuracy: 0.2619\n",
      "Epoch 4/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 2.9237 - accuracy: 0.2696 - val_loss: 2.9057 - val_accuracy: 0.2619\n",
      "Epoch 5/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.8914 - accuracy: 0.2696 - val_loss: 2.8678 - val_accuracy: 0.2619\n",
      "Epoch 6/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 2.8489 - accuracy: 0.2696 - val_loss: 2.8180 - val_accuracy: 0.2619\n",
      "Epoch 7/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 2.7938 - accuracy: 0.2696 - val_loss: 2.7528 - val_accuracy: 0.2619\n",
      "Epoch 8/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 2.7203 - accuracy: 0.2696 - val_loss: 2.6685 - val_accuracy: 0.2619\n",
      "Epoch 9/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.6281 - accuracy: 0.2696 - val_loss: 2.5638 - val_accuracy: 0.2619\n",
      "Epoch 10/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.5182 - accuracy: 0.2696 - val_loss: 2.4441 - val_accuracy: 0.2619\n",
      "Epoch 11/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.3953 - accuracy: 0.2696 - val_loss: 2.3237 - val_accuracy: 0.2619\n",
      "Epoch 12/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.2832 - accuracy: 0.2696 - val_loss: 2.2219 - val_accuracy: 0.2619\n",
      "Epoch 13/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.1963 - accuracy: 0.2696 - val_loss: 2.1510 - val_accuracy: 0.2619\n",
      "Epoch 14/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.1386 - accuracy: 0.2696 - val_loss: 2.1065 - val_accuracy: 0.2619\n",
      "Epoch 15/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.1000 - accuracy: 0.2696 - val_loss: 2.0752 - val_accuracy: 0.2619\n",
      "Epoch 16/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0734 - accuracy: 0.2696 - val_loss: 2.0538 - val_accuracy: 0.2619\n",
      "Epoch 17/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0558 - accuracy: 0.2696 - val_loss: 2.0411 - val_accuracy: 0.2619\n",
      "Epoch 18/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 2.0438 - accuracy: 0.2696 - val_loss: 2.0314 - val_accuracy: 0.2619\n",
      "Epoch 19/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0351 - accuracy: 0.2696 - val_loss: 2.0253 - val_accuracy: 0.2619\n",
      "Epoch 20/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0280 - accuracy: 0.2696 - val_loss: 2.0200 - val_accuracy: 0.2619\n",
      "Epoch 21/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0224 - accuracy: 0.2696 - val_loss: 2.0162 - val_accuracy: 0.2619\n",
      "Epoch 22/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0193 - accuracy: 0.2696 - val_loss: 2.0141 - val_accuracy: 0.2619\n",
      "Epoch 23/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0159 - accuracy: 0.2696 - val_loss: 2.0120 - val_accuracy: 0.2619\n",
      "Epoch 24/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0142 - accuracy: 0.2696 - val_loss: 2.0101 - val_accuracy: 0.2619\n",
      "Epoch 25/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0123 - accuracy: 0.2696 - val_loss: 2.0086 - val_accuracy: 0.2619\n",
      "Epoch 26/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0114 - accuracy: 0.2696 - val_loss: 2.0080 - val_accuracy: 0.2619\n",
      "Epoch 27/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 2.0096 - accuracy: 0.2696 - val_loss: 2.0064 - val_accuracy: 0.2619\n",
      "Epoch 28/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0086 - accuracy: 0.2696 - val_loss: 2.0060 - val_accuracy: 0.2619\n",
      "Epoch 29/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 2.0078 - accuracy: 0.2696 - val_loss: 2.0056 - val_accuracy: 0.2619\n",
      "Epoch 30/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0073 - accuracy: 0.2696 - val_loss: 2.0052 - val_accuracy: 0.2619\n",
      "Epoch 31/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0064 - accuracy: 0.2696 - val_loss: 2.0047 - val_accuracy: 0.2619\n",
      "Epoch 32/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0059 - accuracy: 0.2696 - val_loss: 2.0045 - val_accuracy: 0.2619\n",
      "Epoch 33/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 2.0056 - accuracy: 0.2696 - val_loss: 2.0041 - val_accuracy: 0.2619\n",
      "Epoch 34/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0051 - accuracy: 0.2696 - val_loss: 2.0037 - val_accuracy: 0.2619\n",
      "Epoch 35/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 2.0043 - accuracy: 0.2696 - val_loss: 2.0039 - val_accuracy: 0.2619\n",
      "Epoch 36/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0038 - accuracy: 0.2696 - val_loss: 2.0040 - val_accuracy: 0.2619\n",
      "Epoch 37/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0034 - accuracy: 0.2696 - val_loss: 2.0032 - val_accuracy: 0.2619\n",
      "Epoch 38/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0036 - accuracy: 0.2696 - val_loss: 2.0031 - val_accuracy: 0.2619\n",
      "Epoch 39/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0028 - accuracy: 0.2696 - val_loss: 2.0039 - val_accuracy: 0.2619\n",
      "Epoch 40/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0025 - accuracy: 0.2696 - val_loss: 2.0038 - val_accuracy: 0.2619\n",
      "Epoch 41/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 2.0025 - accuracy: 0.2696 - val_loss: 2.0035 - val_accuracy: 0.2619\n",
      "Epoch 42/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 2.0024 - accuracy: 0.2696 - val_loss: 2.0037 - val_accuracy: 0.2619\n",
      "Epoch 43/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 2.0019 - accuracy: 0.2696 - val_loss: 2.0039 - val_accuracy: 0.2619\n",
      "Epoch 44/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0025 - accuracy: 0.2696 - val_loss: 2.0032 - val_accuracy: 0.2619\n",
      "Epoch 45/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0016 - accuracy: 0.2696 - val_loss: 2.0039 - val_accuracy: 0.2619\n",
      "Epoch 46/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0018 - accuracy: 0.2696 - val_loss: 2.0040 - val_accuracy: 0.2619\n",
      "Epoch 47/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0012 - accuracy: 0.2696 - val_loss: 2.0035 - val_accuracy: 0.2619\n",
      "Epoch 48/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0010 - accuracy: 0.2696 - val_loss: 2.0028 - val_accuracy: 0.2619\n",
      "Epoch 49/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 2.0017 - accuracy: 0.2696 - val_loss: 2.0030 - val_accuracy: 0.2619\n",
      "Epoch 50/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 2.0008 - accuracy: 0.2696 - val_loss: 2.0034 - val_accuracy: 0.2619\n",
      "Epoch 51/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0007 - accuracy: 0.2696 - val_loss: 2.0034 - val_accuracy: 0.2619\n",
      "Epoch 52/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0007 - accuracy: 0.2696 - val_loss: 2.0034 - val_accuracy: 0.2619\n",
      "Epoch 53/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 2.0006 - accuracy: 0.2696 - val_loss: 2.0038 - val_accuracy: 0.2619\n",
      "Epoch 54/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0004 - accuracy: 0.2696 - val_loss: 2.0033 - val_accuracy: 0.2619\n",
      "Epoch 55/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0007 - accuracy: 0.2696 - val_loss: 2.0038 - val_accuracy: 0.2619\n",
      "Epoch 56/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0009 - accuracy: 0.2696 - val_loss: 2.0043 - val_accuracy: 0.2619\n",
      "Epoch 57/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0002 - accuracy: 0.2696 - val_loss: 2.0042 - val_accuracy: 0.2619\n",
      "Epoch 58/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.9998 - accuracy: 0.2696 - val_loss: 2.0038 - val_accuracy: 0.2619\n",
      "Epoch 59/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 2.0002 - accuracy: 0.2696 - val_loss: 2.0034 - val_accuracy: 0.2619\n",
      "Epoch 60/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0006 - accuracy: 0.2696 - val_loss: 2.0037 - val_accuracy: 0.2619\n",
      "Epoch 61/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9997 - accuracy: 0.2696 - val_loss: 2.0037 - val_accuracy: 0.2619\n",
      "Epoch 62/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9999 - accuracy: 0.2696 - val_loss: 2.0044 - val_accuracy: 0.2619\n",
      "Epoch 63/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0000 - accuracy: 0.2696 - val_loss: 2.0045 - val_accuracy: 0.2619\n",
      "Epoch 64/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9998 - accuracy: 0.2696 - val_loss: 2.0041 - val_accuracy: 0.2619\n",
      "Epoch 65/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9997 - accuracy: 0.2696 - val_loss: 2.0043 - val_accuracy: 0.2619\n",
      "Epoch 66/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9996 - accuracy: 0.2696 - val_loss: 2.0043 - val_accuracy: 0.2619\n",
      "Epoch 67/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 2.0004 - accuracy: 0.2696 - val_loss: 2.0043 - val_accuracy: 0.2619\n",
      "Epoch 68/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9998 - accuracy: 0.2696 - val_loss: 2.0042 - val_accuracy: 0.2619\n",
      "Epoch 69/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9996 - accuracy: 0.2696 - val_loss: 2.0044 - val_accuracy: 0.2619\n",
      "Epoch 70/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9994 - accuracy: 0.2696 - val_loss: 2.0049 - val_accuracy: 0.2619\n",
      "Epoch 71/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9997 - accuracy: 0.2696 - val_loss: 2.0044 - val_accuracy: 0.2619\n",
      "Epoch 72/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9996 - accuracy: 0.2696 - val_loss: 2.0050 - val_accuracy: 0.2619\n",
      "Epoch 73/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9993 - accuracy: 0.2696 - val_loss: 2.0047 - val_accuracy: 0.2619\n",
      "Epoch 74/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9996 - accuracy: 0.2696 - val_loss: 2.0051 - val_accuracy: 0.2619\n",
      "Epoch 75/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9996 - accuracy: 0.2696 - val_loss: 2.0051 - val_accuracy: 0.2619\n",
      "Epoch 76/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9995 - accuracy: 0.2696 - val_loss: 2.0050 - val_accuracy: 0.2619\n",
      "Epoch 77/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9993 - accuracy: 0.2696 - val_loss: 2.0045 - val_accuracy: 0.2619\n",
      "Epoch 78/700\n",
      "1176/1176 [==============================] - 0s 73us/step - loss: 1.9992 - accuracy: 0.2696 - val_loss: 2.0049 - val_accuracy: 0.2619\n",
      "Epoch 79/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9994 - accuracy: 0.2696 - val_loss: 2.0042 - val_accuracy: 0.2619\n",
      "Epoch 80/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9992 - accuracy: 0.2696 - val_loss: 2.0042 - val_accuracy: 0.2619\n",
      "Epoch 81/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9990 - accuracy: 0.2696 - val_loss: 2.0039 - val_accuracy: 0.2619\n",
      "Epoch 82/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9992 - accuracy: 0.2696 - val_loss: 2.0038 - val_accuracy: 0.2619\n",
      "Epoch 83/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9987 - accuracy: 0.2696 - val_loss: 2.0044 - val_accuracy: 0.2619\n",
      "Epoch 84/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9993 - accuracy: 0.2696 - val_loss: 2.0045 - val_accuracy: 0.2619\n",
      "Epoch 85/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9984 - accuracy: 0.2696 - val_loss: 2.0048 - val_accuracy: 0.2619\n",
      "Epoch 86/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9988 - accuracy: 0.2696 - val_loss: 2.0048 - val_accuracy: 0.2619\n",
      "Epoch 87/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9986 - accuracy: 0.2696 - val_loss: 2.0046 - val_accuracy: 0.2619\n",
      "Epoch 88/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9990 - accuracy: 0.2696 - val_loss: 2.0048 - val_accuracy: 0.2619\n",
      "Epoch 89/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9986 - accuracy: 0.2696 - val_loss: 2.0047 - val_accuracy: 0.2619\n",
      "Epoch 90/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9987 - accuracy: 0.2696 - val_loss: 2.0041 - val_accuracy: 0.2619\n",
      "Epoch 91/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9986 - accuracy: 0.2696 - val_loss: 2.0044 - val_accuracy: 0.2619\n",
      "Epoch 92/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9990 - accuracy: 0.2696 - val_loss: 2.0044 - val_accuracy: 0.2619\n",
      "Epoch 93/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9989 - accuracy: 0.2696 - val_loss: 2.0049 - val_accuracy: 0.2619\n",
      "Epoch 94/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9983 - accuracy: 0.2696 - val_loss: 2.0048 - val_accuracy: 0.2619\n",
      "Epoch 95/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9985 - accuracy: 0.2696 - val_loss: 2.0052 - val_accuracy: 0.2619\n",
      "Epoch 96/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9984 - accuracy: 0.2696 - val_loss: 2.0050 - val_accuracy: 0.2619\n",
      "Epoch 97/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9988 - accuracy: 0.2696 - val_loss: 2.0048 - val_accuracy: 0.2619\n",
      "Epoch 98/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9981 - accuracy: 0.2696 - val_loss: 2.0054 - val_accuracy: 0.2619\n",
      "Epoch 99/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9980 - accuracy: 0.2696 - val_loss: 2.0049 - val_accuracy: 0.2619\n",
      "Epoch 100/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9985 - accuracy: 0.2696 - val_loss: 2.0051 - val_accuracy: 0.2619\n",
      "Epoch 101/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9986 - accuracy: 0.2696 - val_loss: 2.0049 - val_accuracy: 0.2619\n",
      "Epoch 102/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.9981 - accuracy: 0.2696 - val_loss: 2.0045 - val_accuracy: 0.2619\n",
      "Epoch 103/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9984 - accuracy: 0.2696 - val_loss: 2.0046 - val_accuracy: 0.2619\n",
      "Epoch 104/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.9983 - accuracy: 0.2696 - val_loss: 2.0044 - val_accuracy: 0.2619\n",
      "Epoch 105/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.9983 - accuracy: 0.2696 - val_loss: 2.0045 - val_accuracy: 0.2619\n",
      "Epoch 106/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9979 - accuracy: 0.2696 - val_loss: 2.0043 - val_accuracy: 0.2619\n",
      "Epoch 107/700\n",
      "1176/1176 [==============================] - 0s 82us/step - loss: 1.9984 - accuracy: 0.2696 - val_loss: 2.0043 - val_accuracy: 0.2619\n",
      "Epoch 108/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9981 - accuracy: 0.2696 - val_loss: 2.0047 - val_accuracy: 0.2619\n",
      "Epoch 109/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9978 - accuracy: 0.2696 - val_loss: 2.0046 - val_accuracy: 0.2619\n",
      "Epoch 110/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9979 - accuracy: 0.2696 - val_loss: 2.0050 - val_accuracy: 0.2619\n",
      "Epoch 111/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9979 - accuracy: 0.2696 - val_loss: 2.0051 - val_accuracy: 0.2619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9982 - accuracy: 0.2696 - val_loss: 2.0050 - val_accuracy: 0.2619\n",
      "Epoch 113/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9981 - accuracy: 0.2696 - val_loss: 2.0051 - val_accuracy: 0.2619\n",
      "Epoch 114/700\n",
      "1176/1176 [==============================] - 0s 79us/step - loss: 1.9977 - accuracy: 0.2696 - val_loss: 2.0050 - val_accuracy: 0.2619\n",
      "Epoch 115/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9977 - accuracy: 0.2696 - val_loss: 2.0053 - val_accuracy: 0.2619\n",
      "Epoch 116/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9981 - accuracy: 0.2696 - val_loss: 2.0052 - val_accuracy: 0.2619\n",
      "Epoch 117/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9979 - accuracy: 0.2696 - val_loss: 2.0057 - val_accuracy: 0.2619\n",
      "Epoch 118/700\n",
      "1176/1176 [==============================] - 0s 73us/step - loss: 1.9980 - accuracy: 0.2696 - val_loss: 2.0054 - val_accuracy: 0.2619\n",
      "Epoch 119/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.9982 - accuracy: 0.2696 - val_loss: 2.0056 - val_accuracy: 0.2619\n",
      "Epoch 120/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9977 - accuracy: 0.2696 - val_loss: 2.0050 - val_accuracy: 0.2619\n",
      "Epoch 121/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9974 - accuracy: 0.2696 - val_loss: 2.0048 - val_accuracy: 0.2619\n",
      "Epoch 122/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9977 - accuracy: 0.2696 - val_loss: 2.0049 - val_accuracy: 0.2619\n",
      "Epoch 123/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9970 - accuracy: 0.2696 - val_loss: 2.0048 - val_accuracy: 0.2619\n",
      "Epoch 124/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9974 - accuracy: 0.2696 - val_loss: 2.0045 - val_accuracy: 0.2619\n",
      "Epoch 125/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9977 - accuracy: 0.2696 - val_loss: 2.0048 - val_accuracy: 0.2619\n",
      "Epoch 126/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.9976 - accuracy: 0.2696 - val_loss: 2.0046 - val_accuracy: 0.2619\n",
      "Epoch 127/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9974 - accuracy: 0.2696 - val_loss: 2.0046 - val_accuracy: 0.2619\n",
      "Epoch 128/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9971 - accuracy: 0.2696 - val_loss: 2.0043 - val_accuracy: 0.2619\n",
      "Epoch 129/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.9969 - accuracy: 0.2696 - val_loss: 2.0041 - val_accuracy: 0.2619\n",
      "Epoch 130/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9968 - accuracy: 0.2696 - val_loss: 2.0043 - val_accuracy: 0.2619\n",
      "Epoch 131/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9972 - accuracy: 0.2696 - val_loss: 2.0045 - val_accuracy: 0.2619\n",
      "Epoch 132/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9971 - accuracy: 0.2696 - val_loss: 2.0043 - val_accuracy: 0.2619\n",
      "Epoch 133/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9974 - accuracy: 0.2696 - val_loss: 2.0045 - val_accuracy: 0.2619\n",
      "Epoch 134/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9971 - accuracy: 0.2696 - val_loss: 2.0041 - val_accuracy: 0.2619\n",
      "Epoch 135/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9968 - accuracy: 0.2696 - val_loss: 2.0039 - val_accuracy: 0.2619\n",
      "Epoch 136/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9964 - accuracy: 0.2696 - val_loss: 2.0041 - val_accuracy: 0.2619\n",
      "Epoch 137/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9963 - accuracy: 0.2696 - val_loss: 2.0040 - val_accuracy: 0.2619\n",
      "Epoch 138/700\n",
      "1176/1176 [==============================] - 0s 75us/step - loss: 1.9963 - accuracy: 0.2696 - val_loss: 2.0040 - val_accuracy: 0.2619\n",
      "Epoch 139/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.9962 - accuracy: 0.2696 - val_loss: 2.0035 - val_accuracy: 0.2619\n",
      "Epoch 140/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9963 - accuracy: 0.2696 - val_loss: 2.0037 - val_accuracy: 0.2619\n",
      "Epoch 141/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.9964 - accuracy: 0.2696 - val_loss: 2.0035 - val_accuracy: 0.2619\n",
      "Epoch 142/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9960 - accuracy: 0.2696 - val_loss: 2.0037 - val_accuracy: 0.2619\n",
      "Epoch 143/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9959 - accuracy: 0.2696 - val_loss: 2.0040 - val_accuracy: 0.2619\n",
      "Epoch 144/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9956 - accuracy: 0.2696 - val_loss: 2.0040 - val_accuracy: 0.2619\n",
      "Epoch 145/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9961 - accuracy: 0.2696 - val_loss: 2.0040 - val_accuracy: 0.2619\n",
      "Epoch 146/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9955 - accuracy: 0.2696 - val_loss: 2.0041 - val_accuracy: 0.2619\n",
      "Epoch 147/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.9957 - accuracy: 0.2696 - val_loss: 2.0035 - val_accuracy: 0.2619\n",
      "Epoch 148/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.9957 - accuracy: 0.2696 - val_loss: 2.0031 - val_accuracy: 0.2619\n",
      "Epoch 149/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9950 - accuracy: 0.2696 - val_loss: 2.0033 - val_accuracy: 0.2619\n",
      "Epoch 150/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9951 - accuracy: 0.2696 - val_loss: 2.0030 - val_accuracy: 0.2619\n",
      "Epoch 151/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9947 - accuracy: 0.2696 - val_loss: 2.0027 - val_accuracy: 0.2619\n",
      "Epoch 152/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9946 - accuracy: 0.2696 - val_loss: 2.0027 - val_accuracy: 0.2619\n",
      "Epoch 153/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9948 - accuracy: 0.2696 - val_loss: 2.0022 - val_accuracy: 0.2619\n",
      "Epoch 154/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9943 - accuracy: 0.2696 - val_loss: 2.0020 - val_accuracy: 0.2619\n",
      "Epoch 155/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9940 - accuracy: 0.2696 - val_loss: 2.0018 - val_accuracy: 0.2619\n",
      "Epoch 156/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9937 - accuracy: 0.2696 - val_loss: 2.0014 - val_accuracy: 0.2619\n",
      "Epoch 157/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9936 - accuracy: 0.2696 - val_loss: 2.0011 - val_accuracy: 0.2619\n",
      "Epoch 158/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9936 - accuracy: 0.2696 - val_loss: 2.0009 - val_accuracy: 0.2619\n",
      "Epoch 159/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9931 - accuracy: 0.2696 - val_loss: 2.0008 - val_accuracy: 0.2619\n",
      "Epoch 160/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9931 - accuracy: 0.2696 - val_loss: 2.0005 - val_accuracy: 0.2619\n",
      "Epoch 161/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9924 - accuracy: 0.2696 - val_loss: 2.0003 - val_accuracy: 0.2619\n",
      "Epoch 162/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9924 - accuracy: 0.2696 - val_loss: 2.0000 - val_accuracy: 0.2619\n",
      "Epoch 163/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9918 - accuracy: 0.2696 - val_loss: 1.9997 - val_accuracy: 0.2619\n",
      "Epoch 164/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9914 - accuracy: 0.2696 - val_loss: 1.9991 - val_accuracy: 0.2619\n",
      "Epoch 165/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9905 - accuracy: 0.2696 - val_loss: 1.9982 - val_accuracy: 0.2619\n",
      "Epoch 166/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9906 - accuracy: 0.2696 - val_loss: 1.9978 - val_accuracy: 0.2619\n",
      "Epoch 167/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9900 - accuracy: 0.2696 - val_loss: 1.9980 - val_accuracy: 0.2619\n",
      "Epoch 168/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9891 - accuracy: 0.2696 - val_loss: 1.9975 - val_accuracy: 0.2619\n",
      "Epoch 169/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9890 - accuracy: 0.2696 - val_loss: 1.9965 - val_accuracy: 0.2619\n",
      "Epoch 170/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9882 - accuracy: 0.2696 - val_loss: 1.9959 - val_accuracy: 0.2619\n",
      "Epoch 171/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9875 - accuracy: 0.2696 - val_loss: 1.9954 - val_accuracy: 0.2619\n",
      "Epoch 172/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9869 - accuracy: 0.2696 - val_loss: 1.9944 - val_accuracy: 0.2619\n",
      "Epoch 173/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9864 - accuracy: 0.2696 - val_loss: 1.9940 - val_accuracy: 0.2619\n",
      "Epoch 174/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9852 - accuracy: 0.2696 - val_loss: 1.9926 - val_accuracy: 0.2619\n",
      "Epoch 175/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9839 - accuracy: 0.2696 - val_loss: 1.9915 - val_accuracy: 0.2619\n",
      "Epoch 176/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9833 - accuracy: 0.2696 - val_loss: 1.9910 - val_accuracy: 0.2619\n",
      "Epoch 177/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9821 - accuracy: 0.2696 - val_loss: 1.9897 - val_accuracy: 0.2619\n",
      "Epoch 178/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9810 - accuracy: 0.2696 - val_loss: 1.9892 - val_accuracy: 0.2619\n",
      "Epoch 179/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9799 - accuracy: 0.2696 - val_loss: 1.9873 - val_accuracy: 0.2619\n",
      "Epoch 180/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9791 - accuracy: 0.2696 - val_loss: 1.9867 - val_accuracy: 0.2619\n",
      "Epoch 181/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9776 - accuracy: 0.2696 - val_loss: 1.9847 - val_accuracy: 0.2619\n",
      "Epoch 182/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9763 - accuracy: 0.2696 - val_loss: 1.9831 - val_accuracy: 0.2619\n",
      "Epoch 183/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.9746 - accuracy: 0.2696 - val_loss: 1.9818 - val_accuracy: 0.2619\n",
      "Epoch 184/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9730 - accuracy: 0.2696 - val_loss: 1.9802 - val_accuracy: 0.2619\n",
      "Epoch 185/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9714 - accuracy: 0.2696 - val_loss: 1.9781 - val_accuracy: 0.2619\n",
      "Epoch 186/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9693 - accuracy: 0.2696 - val_loss: 1.9770 - val_accuracy: 0.2619\n",
      "Epoch 187/700\n",
      "1176/1176 [==============================] - 0s 77us/step - loss: 1.9677 - accuracy: 0.2696 - val_loss: 1.9750 - val_accuracy: 0.2619\n",
      "Epoch 188/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9657 - accuracy: 0.2696 - val_loss: 1.9734 - val_accuracy: 0.2619\n",
      "Epoch 189/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9634 - accuracy: 0.2696 - val_loss: 1.9713 - val_accuracy: 0.2619\n",
      "Epoch 190/700\n",
      "1176/1176 [==============================] - 0s 76us/step - loss: 1.9612 - accuracy: 0.2696 - val_loss: 1.9689 - val_accuracy: 0.2619\n",
      "Epoch 191/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9589 - accuracy: 0.2696 - val_loss: 1.9659 - val_accuracy: 0.2619\n",
      "Epoch 192/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9566 - accuracy: 0.2696 - val_loss: 1.9635 - val_accuracy: 0.2619\n",
      "Epoch 193/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9536 - accuracy: 0.2696 - val_loss: 1.9608 - val_accuracy: 0.2619\n",
      "Epoch 194/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9519 - accuracy: 0.2696 - val_loss: 1.9597 - val_accuracy: 0.2619\n",
      "Epoch 195/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9494 - accuracy: 0.2696 - val_loss: 1.9559 - val_accuracy: 0.2619\n",
      "Epoch 196/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9469 - accuracy: 0.2696 - val_loss: 1.9526 - val_accuracy: 0.2619\n",
      "Epoch 197/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9435 - accuracy: 0.2679 - val_loss: 1.9501 - val_accuracy: 0.2619\n",
      "Epoch 198/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9408 - accuracy: 0.2679 - val_loss: 1.9482 - val_accuracy: 0.2619\n",
      "Epoch 199/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9382 - accuracy: 0.2687 - val_loss: 1.9446 - val_accuracy: 0.2639\n",
      "Epoch 200/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9352 - accuracy: 0.2704 - val_loss: 1.9421 - val_accuracy: 0.2639\n",
      "Epoch 201/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.9321 - accuracy: 0.2679 - val_loss: 1.9388 - val_accuracy: 0.2659\n",
      "Epoch 202/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.9293 - accuracy: 0.2713 - val_loss: 1.9363 - val_accuracy: 0.2659\n",
      "Epoch 203/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.9264 - accuracy: 0.2713 - val_loss: 1.9334 - val_accuracy: 0.2639\n",
      "Epoch 204/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.9230 - accuracy: 0.2730 - val_loss: 1.9304 - val_accuracy: 0.2639\n",
      "Epoch 205/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9219 - accuracy: 0.2738 - val_loss: 1.9273 - val_accuracy: 0.2659\n",
      "Epoch 206/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9177 - accuracy: 0.2747 - val_loss: 1.9241 - val_accuracy: 0.2659\n",
      "Epoch 207/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.9156 - accuracy: 0.2789 - val_loss: 1.9213 - val_accuracy: 0.2738\n",
      "Epoch 208/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9121 - accuracy: 0.2730 - val_loss: 1.9213 - val_accuracy: 0.2659\n",
      "Epoch 209/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.9118 - accuracy: 0.2781 - val_loss: 1.9166 - val_accuracy: 0.2738\n",
      "Epoch 210/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.9081 - accuracy: 0.2806 - val_loss: 1.9141 - val_accuracy: 0.2857\n",
      "Epoch 211/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.9051 - accuracy: 0.2764 - val_loss: 1.9121 - val_accuracy: 0.2758\n",
      "Epoch 212/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.9044 - accuracy: 0.2747 - val_loss: 1.9087 - val_accuracy: 0.2837\n",
      "Epoch 213/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.9023 - accuracy: 0.2832 - val_loss: 1.9082 - val_accuracy: 0.2837\n",
      "Epoch 214/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8996 - accuracy: 0.2721 - val_loss: 1.9052 - val_accuracy: 0.2778\n",
      "Epoch 215/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8975 - accuracy: 0.2781 - val_loss: 1.9052 - val_accuracy: 0.2817\n",
      "Epoch 216/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8968 - accuracy: 0.2764 - val_loss: 1.9010 - val_accuracy: 0.2837\n",
      "Epoch 217/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8945 - accuracy: 0.2772 - val_loss: 1.9000 - val_accuracy: 0.2857\n",
      "Epoch 218/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8929 - accuracy: 0.2806 - val_loss: 1.8981 - val_accuracy: 0.2758\n",
      "Epoch 219/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8906 - accuracy: 0.2764 - val_loss: 1.8959 - val_accuracy: 0.2857\n",
      "Epoch 220/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8900 - accuracy: 0.2781 - val_loss: 1.8941 - val_accuracy: 0.2897\n",
      "Epoch 221/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8870 - accuracy: 0.2840 - val_loss: 1.8934 - val_accuracy: 0.2837\n",
      "Epoch 222/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8857 - accuracy: 0.2772 - val_loss: 1.8909 - val_accuracy: 0.2897\n",
      "Epoch 223/700\n",
      "1176/1176 [==============================] - 0s 75us/step - loss: 1.8846 - accuracy: 0.2789 - val_loss: 1.8896 - val_accuracy: 0.2917\n",
      "Epoch 224/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8826 - accuracy: 0.2772 - val_loss: 1.8887 - val_accuracy: 0.2877\n",
      "Epoch 225/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8809 - accuracy: 0.2772 - val_loss: 1.8870 - val_accuracy: 0.2857\n",
      "Epoch 226/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8789 - accuracy: 0.2747 - val_loss: 1.8856 - val_accuracy: 0.2778\n",
      "Epoch 227/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8794 - accuracy: 0.2806 - val_loss: 1.8839 - val_accuracy: 0.2798\n",
      "Epoch 228/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8771 - accuracy: 0.2798 - val_loss: 1.8856 - val_accuracy: 0.2817\n",
      "Epoch 229/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8764 - accuracy: 0.2789 - val_loss: 1.8828 - val_accuracy: 0.2817\n",
      "Epoch 230/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.8747 - accuracy: 0.2755 - val_loss: 1.8810 - val_accuracy: 0.2798\n",
      "Epoch 231/700\n",
      "1176/1176 [==============================] - 0s 77us/step - loss: 1.8734 - accuracy: 0.2832 - val_loss: 1.8814 - val_accuracy: 0.2817\n",
      "Epoch 232/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8719 - accuracy: 0.2789 - val_loss: 1.8797 - val_accuracy: 0.2778\n",
      "Epoch 233/700\n",
      "1176/1176 [==============================] - 0s 78us/step - loss: 1.8726 - accuracy: 0.2789 - val_loss: 1.8784 - val_accuracy: 0.2817\n",
      "Epoch 234/700\n",
      "1176/1176 [==============================] - 0s 78us/step - loss: 1.8714 - accuracy: 0.2755 - val_loss: 1.8762 - val_accuracy: 0.2877\n",
      "Epoch 235/700\n",
      "1176/1176 [==============================] - 0s 75us/step - loss: 1.8693 - accuracy: 0.2772 - val_loss: 1.8759 - val_accuracy: 0.2817\n",
      "Epoch 236/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8688 - accuracy: 0.2781 - val_loss: 1.8747 - val_accuracy: 0.2837\n",
      "Epoch 237/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8675 - accuracy: 0.2747 - val_loss: 1.8741 - val_accuracy: 0.2877\n",
      "Epoch 238/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8668 - accuracy: 0.2823 - val_loss: 1.8744 - val_accuracy: 0.2937\n",
      "Epoch 239/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8664 - accuracy: 0.2781 - val_loss: 1.8723 - val_accuracy: 0.2897\n",
      "Epoch 240/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8656 - accuracy: 0.2815 - val_loss: 1.8719 - val_accuracy: 0.2917\n",
      "Epoch 241/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8626 - accuracy: 0.2764 - val_loss: 1.8747 - val_accuracy: 0.2917\n",
      "Epoch 242/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8634 - accuracy: 0.2798 - val_loss: 1.8711 - val_accuracy: 0.2897\n",
      "Epoch 243/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8619 - accuracy: 0.2789 - val_loss: 1.8698 - val_accuracy: 0.2798\n",
      "Epoch 244/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8616 - accuracy: 0.2781 - val_loss: 1.8690 - val_accuracy: 0.2837\n",
      "Epoch 245/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8607 - accuracy: 0.2764 - val_loss: 1.8698 - val_accuracy: 0.2897\n",
      "Epoch 246/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8613 - accuracy: 0.2747 - val_loss: 1.8676 - val_accuracy: 0.2976\n",
      "Epoch 247/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8597 - accuracy: 0.2764 - val_loss: 1.8692 - val_accuracy: 0.2897\n",
      "Epoch 248/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8589 - accuracy: 0.2806 - val_loss: 1.8689 - val_accuracy: 0.2877\n",
      "Epoch 249/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8593 - accuracy: 0.2908 - val_loss: 1.8694 - val_accuracy: 0.3056\n",
      "Epoch 250/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8588 - accuracy: 0.3027 - val_loss: 1.8670 - val_accuracy: 0.3016\n",
      "Epoch 251/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8571 - accuracy: 0.2993 - val_loss: 1.8674 - val_accuracy: 0.3095\n",
      "Epoch 252/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8585 - accuracy: 0.2968 - val_loss: 1.8654 - val_accuracy: 0.3056\n",
      "Epoch 253/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8564 - accuracy: 0.3036 - val_loss: 1.8650 - val_accuracy: 0.3056\n",
      "Epoch 254/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8554 - accuracy: 0.3044 - val_loss: 1.8646 - val_accuracy: 0.3095\n",
      "Epoch 255/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8563 - accuracy: 0.2968 - val_loss: 1.8640 - val_accuracy: 0.3075\n",
      "Epoch 256/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8535 - accuracy: 0.3027 - val_loss: 1.8668 - val_accuracy: 0.2996\n",
      "Epoch 257/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8547 - accuracy: 0.3095 - val_loss: 1.8629 - val_accuracy: 0.3036\n",
      "Epoch 258/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8541 - accuracy: 0.3163 - val_loss: 1.8624 - val_accuracy: 0.3095\n",
      "Epoch 259/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8531 - accuracy: 0.3095 - val_loss: 1.8618 - val_accuracy: 0.3016\n",
      "Epoch 260/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8521 - accuracy: 0.3070 - val_loss: 1.8616 - val_accuracy: 0.3016\n",
      "Epoch 261/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8529 - accuracy: 0.3053 - val_loss: 1.8621 - val_accuracy: 0.3016\n",
      "Epoch 262/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8517 - accuracy: 0.3019 - val_loss: 1.8615 - val_accuracy: 0.3016\n",
      "Epoch 263/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8531 - accuracy: 0.3138 - val_loss: 1.8606 - val_accuracy: 0.3075\n",
      "Epoch 264/700\n",
      "1176/1176 [==============================] - 0s 73us/step - loss: 1.8508 - accuracy: 0.3053 - val_loss: 1.8604 - val_accuracy: 0.3016\n",
      "Epoch 265/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8507 - accuracy: 0.3104 - val_loss: 1.8623 - val_accuracy: 0.2956\n",
      "Epoch 266/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8499 - accuracy: 0.3155 - val_loss: 1.8601 - val_accuracy: 0.2976\n",
      "Epoch 267/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8497 - accuracy: 0.3121 - val_loss: 1.8593 - val_accuracy: 0.2976\n",
      "Epoch 268/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8492 - accuracy: 0.3146 - val_loss: 1.8596 - val_accuracy: 0.2956\n",
      "Epoch 269/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8500 - accuracy: 0.3129 - val_loss: 1.8596 - val_accuracy: 0.2996\n",
      "Epoch 270/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8495 - accuracy: 0.3180 - val_loss: 1.8592 - val_accuracy: 0.3016\n",
      "Epoch 271/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8479 - accuracy: 0.3172 - val_loss: 1.8592 - val_accuracy: 0.3095\n",
      "Epoch 272/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8468 - accuracy: 0.3163 - val_loss: 1.8597 - val_accuracy: 0.3095\n",
      "Epoch 273/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8487 - accuracy: 0.3155 - val_loss: 1.8574 - val_accuracy: 0.3016\n",
      "Epoch 274/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8468 - accuracy: 0.3180 - val_loss: 1.8570 - val_accuracy: 0.2976\n",
      "Epoch 275/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8472 - accuracy: 0.3172 - val_loss: 1.8577 - val_accuracy: 0.3115\n",
      "Epoch 276/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8458 - accuracy: 0.3197 - val_loss: 1.8565 - val_accuracy: 0.3155\n",
      "Epoch 277/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8449 - accuracy: 0.3138 - val_loss: 1.8560 - val_accuracy: 0.3115\n",
      "Epoch 278/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8442 - accuracy: 0.3197 - val_loss: 1.8577 - val_accuracy: 0.3075\n",
      "Epoch 279/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8436 - accuracy: 0.3197 - val_loss: 1.8555 - val_accuracy: 0.3036\n",
      "Epoch 280/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8440 - accuracy: 0.3163 - val_loss: 1.8555 - val_accuracy: 0.3115\n",
      "Epoch 281/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8453 - accuracy: 0.3129 - val_loss: 1.8549 - val_accuracy: 0.3016\n",
      "Epoch 282/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.8429 - accuracy: 0.3189 - val_loss: 1.8558 - val_accuracy: 0.3095\n",
      "Epoch 283/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8433 - accuracy: 0.3214 - val_loss: 1.8544 - val_accuracy: 0.3115\n",
      "Epoch 284/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8425 - accuracy: 0.3189 - val_loss: 1.8537 - val_accuracy: 0.3056\n",
      "Epoch 285/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8421 - accuracy: 0.3265 - val_loss: 1.8538 - val_accuracy: 0.3095\n",
      "Epoch 286/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8423 - accuracy: 0.3121 - val_loss: 1.8536 - val_accuracy: 0.3135\n",
      "Epoch 287/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8429 - accuracy: 0.3146 - val_loss: 1.8533 - val_accuracy: 0.3095\n",
      "Epoch 288/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8408 - accuracy: 0.3121 - val_loss: 1.8550 - val_accuracy: 0.3036\n",
      "Epoch 289/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8396 - accuracy: 0.3223 - val_loss: 1.8525 - val_accuracy: 0.3075\n",
      "Epoch 290/700\n",
      "1176/1176 [==============================] - 0s 73us/step - loss: 1.8393 - accuracy: 0.3223 - val_loss: 1.8550 - val_accuracy: 0.3056\n",
      "Epoch 291/700\n",
      "1176/1176 [==============================] - 0s 84us/step - loss: 1.8398 - accuracy: 0.3172 - val_loss: 1.8516 - val_accuracy: 0.3135\n",
      "Epoch 292/700\n",
      "1176/1176 [==============================] - 0s 95us/step - loss: 1.8386 - accuracy: 0.3265 - val_loss: 1.8513 - val_accuracy: 0.3016\n",
      "Epoch 293/700\n",
      "1176/1176 [==============================] - 0s 86us/step - loss: 1.8389 - accuracy: 0.3257 - val_loss: 1.8545 - val_accuracy: 0.3095\n",
      "Epoch 294/700\n",
      "1176/1176 [==============================] - 0s 89us/step - loss: 1.8385 - accuracy: 0.3223 - val_loss: 1.8499 - val_accuracy: 0.3075\n",
      "Epoch 295/700\n",
      "1176/1176 [==============================] - 0s 75us/step - loss: 1.8389 - accuracy: 0.3223 - val_loss: 1.8505 - val_accuracy: 0.3135\n",
      "Epoch 296/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.8378 - accuracy: 0.3240 - val_loss: 1.8514 - val_accuracy: 0.3075\n",
      "Epoch 297/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8369 - accuracy: 0.3163 - val_loss: 1.8493 - val_accuracy: 0.3115\n",
      "Epoch 298/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8370 - accuracy: 0.3240 - val_loss: 1.8496 - val_accuracy: 0.3135\n",
      "Epoch 299/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8361 - accuracy: 0.3206 - val_loss: 1.8479 - val_accuracy: 0.3075\n",
      "Epoch 300/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8373 - accuracy: 0.3180 - val_loss: 1.8478 - val_accuracy: 0.3095\n",
      "Epoch 301/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8353 - accuracy: 0.3248 - val_loss: 1.8477 - val_accuracy: 0.3056\n",
      "Epoch 302/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8341 - accuracy: 0.3231 - val_loss: 1.8512 - val_accuracy: 0.3115\n",
      "Epoch 303/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.8361 - accuracy: 0.3257 - val_loss: 1.8467 - val_accuracy: 0.3036\n",
      "Epoch 304/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8346 - accuracy: 0.3214 - val_loss: 1.8484 - val_accuracy: 0.3155\n",
      "Epoch 305/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8325 - accuracy: 0.3291 - val_loss: 1.8462 - val_accuracy: 0.3115\n",
      "Epoch 306/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8334 - accuracy: 0.3248 - val_loss: 1.8457 - val_accuracy: 0.3056\n",
      "Epoch 307/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8338 - accuracy: 0.3308 - val_loss: 1.8469 - val_accuracy: 0.3135\n",
      "Epoch 308/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8333 - accuracy: 0.3257 - val_loss: 1.8466 - val_accuracy: 0.3115\n",
      "Epoch 309/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8318 - accuracy: 0.3248 - val_loss: 1.8455 - val_accuracy: 0.3095\n",
      "Epoch 310/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8325 - accuracy: 0.3214 - val_loss: 1.8480 - val_accuracy: 0.3115\n",
      "Epoch 311/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8321 - accuracy: 0.3214 - val_loss: 1.8439 - val_accuracy: 0.3056\n",
      "Epoch 312/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8308 - accuracy: 0.3197 - val_loss: 1.8437 - val_accuracy: 0.3075\n",
      "Epoch 313/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8309 - accuracy: 0.3274 - val_loss: 1.8436 - val_accuracy: 0.3075\n",
      "Epoch 314/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8304 - accuracy: 0.3257 - val_loss: 1.8432 - val_accuracy: 0.3135\n",
      "Epoch 315/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8299 - accuracy: 0.3274 - val_loss: 1.8473 - val_accuracy: 0.3115\n",
      "Epoch 316/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8305 - accuracy: 0.3240 - val_loss: 1.8440 - val_accuracy: 0.3135\n",
      "Epoch 317/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8292 - accuracy: 0.3274 - val_loss: 1.8422 - val_accuracy: 0.3095\n",
      "Epoch 318/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8290 - accuracy: 0.3282 - val_loss: 1.8414 - val_accuracy: 0.3056\n",
      "Epoch 319/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8282 - accuracy: 0.3248 - val_loss: 1.8414 - val_accuracy: 0.3095\n",
      "Epoch 320/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8275 - accuracy: 0.3282 - val_loss: 1.8435 - val_accuracy: 0.3194\n",
      "Epoch 321/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8279 - accuracy: 0.3248 - val_loss: 1.8407 - val_accuracy: 0.2996\n",
      "Epoch 322/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8292 - accuracy: 0.3172 - val_loss: 1.8404 - val_accuracy: 0.3016\n",
      "Epoch 323/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8278 - accuracy: 0.3231 - val_loss: 1.8410 - val_accuracy: 0.3075\n",
      "Epoch 324/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8277 - accuracy: 0.3223 - val_loss: 1.8397 - val_accuracy: 0.3075\n",
      "Epoch 325/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8267 - accuracy: 0.3231 - val_loss: 1.8408 - val_accuracy: 0.3115\n",
      "Epoch 326/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8261 - accuracy: 0.3274 - val_loss: 1.8386 - val_accuracy: 0.3036\n",
      "Epoch 327/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8267 - accuracy: 0.3274 - val_loss: 1.8414 - val_accuracy: 0.3155\n",
      "Epoch 328/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8257 - accuracy: 0.3274 - val_loss: 1.8382 - val_accuracy: 0.3056\n",
      "Epoch 329/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8260 - accuracy: 0.3248 - val_loss: 1.8398 - val_accuracy: 0.3095\n",
      "Epoch 330/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8247 - accuracy: 0.3299 - val_loss: 1.8424 - val_accuracy: 0.3135\n",
      "Epoch 331/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8262 - accuracy: 0.3240 - val_loss: 1.8388 - val_accuracy: 0.3075\n",
      "Epoch 332/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 78us/step - loss: 1.8242 - accuracy: 0.3240 - val_loss: 1.8408 - val_accuracy: 0.3095\n",
      "Epoch 333/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8233 - accuracy: 0.3282 - val_loss: 1.8424 - val_accuracy: 0.3095\n",
      "Epoch 334/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8255 - accuracy: 0.3223 - val_loss: 1.8374 - val_accuracy: 0.3036\n",
      "Epoch 335/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8222 - accuracy: 0.3274 - val_loss: 1.8364 - val_accuracy: 0.3036\n",
      "Epoch 336/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8225 - accuracy: 0.3316 - val_loss: 1.8363 - val_accuracy: 0.3075\n",
      "Epoch 337/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8227 - accuracy: 0.3291 - val_loss: 1.8376 - val_accuracy: 0.3155\n",
      "Epoch 338/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8223 - accuracy: 0.3257 - val_loss: 1.8384 - val_accuracy: 0.3115\n",
      "Epoch 339/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8221 - accuracy: 0.3248 - val_loss: 1.8372 - val_accuracy: 0.3135\n",
      "Epoch 340/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8219 - accuracy: 0.3240 - val_loss: 1.8348 - val_accuracy: 0.3095\n",
      "Epoch 341/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8222 - accuracy: 0.3282 - val_loss: 1.8356 - val_accuracy: 0.3075\n",
      "Epoch 342/700\n",
      "1176/1176 [==============================] - 0s 77us/step - loss: 1.8218 - accuracy: 0.3206 - val_loss: 1.8347 - val_accuracy: 0.3095\n",
      "Epoch 343/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8210 - accuracy: 0.3299 - val_loss: 1.8344 - val_accuracy: 0.3135\n",
      "Epoch 344/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8213 - accuracy: 0.3197 - val_loss: 1.8333 - val_accuracy: 0.3095\n",
      "Epoch 345/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8209 - accuracy: 0.3274 - val_loss: 1.8347 - val_accuracy: 0.3115\n",
      "Epoch 346/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8206 - accuracy: 0.3214 - val_loss: 1.8357 - val_accuracy: 0.3155\n",
      "Epoch 347/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8199 - accuracy: 0.3274 - val_loss: 1.8335 - val_accuracy: 0.3095\n",
      "Epoch 348/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8191 - accuracy: 0.3257 - val_loss: 1.8328 - val_accuracy: 0.3115\n",
      "Epoch 349/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8202 - accuracy: 0.3265 - val_loss: 1.8323 - val_accuracy: 0.3056\n",
      "Epoch 350/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8191 - accuracy: 0.3240 - val_loss: 1.8364 - val_accuracy: 0.3115\n",
      "Epoch 351/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8194 - accuracy: 0.3316 - val_loss: 1.8342 - val_accuracy: 0.3175\n",
      "Epoch 352/700\n",
      "1176/1176 [==============================] - 0s 77us/step - loss: 1.8176 - accuracy: 0.3308 - val_loss: 1.8316 - val_accuracy: 0.3036\n",
      "Epoch 353/700\n",
      "1176/1176 [==============================] - 0s 73us/step - loss: 1.8192 - accuracy: 0.3274 - val_loss: 1.8349 - val_accuracy: 0.3135\n",
      "Epoch 354/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8179 - accuracy: 0.3308 - val_loss: 1.8337 - val_accuracy: 0.3175\n",
      "Epoch 355/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.8177 - accuracy: 0.3308 - val_loss: 1.8307 - val_accuracy: 0.3095\n",
      "Epoch 356/700\n",
      "1176/1176 [==============================] - 0s 76us/step - loss: 1.8186 - accuracy: 0.3197 - val_loss: 1.8309 - val_accuracy: 0.3056\n",
      "Epoch 357/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.8173 - accuracy: 0.3231 - val_loss: 1.8326 - val_accuracy: 0.3075\n",
      "Epoch 358/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8167 - accuracy: 0.3248 - val_loss: 1.8313 - val_accuracy: 0.3175\n",
      "Epoch 359/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8184 - accuracy: 0.3257 - val_loss: 1.8312 - val_accuracy: 0.3056\n",
      "Epoch 360/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8176 - accuracy: 0.3299 - val_loss: 1.8307 - val_accuracy: 0.3056\n",
      "Epoch 361/700\n",
      "1176/1176 [==============================] - 0s 80us/step - loss: 1.8165 - accuracy: 0.3257 - val_loss: 1.8349 - val_accuracy: 0.3115\n",
      "Epoch 362/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8158 - accuracy: 0.3257 - val_loss: 1.8298 - val_accuracy: 0.3095\n",
      "Epoch 363/700\n",
      "1176/1176 [==============================] - 0s 79us/step - loss: 1.8162 - accuracy: 0.3248 - val_loss: 1.8302 - val_accuracy: 0.3155\n",
      "Epoch 364/700\n",
      "1176/1176 [==============================] - 0s 79us/step - loss: 1.8163 - accuracy: 0.3197 - val_loss: 1.8305 - val_accuracy: 0.3075\n",
      "Epoch 365/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8150 - accuracy: 0.3257 - val_loss: 1.8295 - val_accuracy: 0.3155\n",
      "Epoch 366/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8153 - accuracy: 0.3265 - val_loss: 1.8301 - val_accuracy: 0.3175\n",
      "Epoch 367/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8152 - accuracy: 0.3206 - val_loss: 1.8310 - val_accuracy: 0.3135\n",
      "Epoch 368/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8149 - accuracy: 0.3231 - val_loss: 1.8293 - val_accuracy: 0.3155\n",
      "Epoch 369/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.8160 - accuracy: 0.3197 - val_loss: 1.8297 - val_accuracy: 0.3115\n",
      "Epoch 370/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8152 - accuracy: 0.3265 - val_loss: 1.8310 - val_accuracy: 0.3135\n",
      "Epoch 371/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8148 - accuracy: 0.3282 - val_loss: 1.8321 - val_accuracy: 0.3135\n",
      "Epoch 372/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8149 - accuracy: 0.3265 - val_loss: 1.8282 - val_accuracy: 0.3135\n",
      "Epoch 373/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8140 - accuracy: 0.3316 - val_loss: 1.8277 - val_accuracy: 0.3115\n",
      "Epoch 374/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8136 - accuracy: 0.3240 - val_loss: 1.8287 - val_accuracy: 0.3075\n",
      "Epoch 375/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8139 - accuracy: 0.3248 - val_loss: 1.8271 - val_accuracy: 0.3095\n",
      "Epoch 376/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8135 - accuracy: 0.3248 - val_loss: 1.8286 - val_accuracy: 0.3135\n",
      "Epoch 377/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8130 - accuracy: 0.3223 - val_loss: 1.8277 - val_accuracy: 0.3115\n",
      "Epoch 378/700\n",
      "1176/1176 [==============================] - 0s 77us/step - loss: 1.8132 - accuracy: 0.3197 - val_loss: 1.8301 - val_accuracy: 0.3135\n",
      "Epoch 379/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8136 - accuracy: 0.3214 - val_loss: 1.8295 - val_accuracy: 0.3095\n",
      "Epoch 380/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8135 - accuracy: 0.3248 - val_loss: 1.8282 - val_accuracy: 0.3075\n",
      "Epoch 381/700\n",
      "1176/1176 [==============================] - 0s 76us/step - loss: 1.8128 - accuracy: 0.3223 - val_loss: 1.8274 - val_accuracy: 0.3095\n",
      "Epoch 382/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.8129 - accuracy: 0.3299 - val_loss: 1.8266 - val_accuracy: 0.3135\n",
      "Epoch 383/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8120 - accuracy: 0.3248 - val_loss: 1.8266 - val_accuracy: 0.3115\n",
      "Epoch 384/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8128 - accuracy: 0.3291 - val_loss: 1.8269 - val_accuracy: 0.3135\n",
      "Epoch 385/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8125 - accuracy: 0.3214 - val_loss: 1.8277 - val_accuracy: 0.3095\n",
      "Epoch 386/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8118 - accuracy: 0.3214 - val_loss: 1.8275 - val_accuracy: 0.3135\n",
      "Epoch 387/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8110 - accuracy: 0.3248 - val_loss: 1.8267 - val_accuracy: 0.3115\n",
      "Epoch 388/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8118 - accuracy: 0.3282 - val_loss: 1.8267 - val_accuracy: 0.3095\n",
      "Epoch 389/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8114 - accuracy: 0.3206 - val_loss: 1.8263 - val_accuracy: 0.3115\n",
      "Epoch 390/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8112 - accuracy: 0.3240 - val_loss: 1.8256 - val_accuracy: 0.3115\n",
      "Epoch 391/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8105 - accuracy: 0.3282 - val_loss: 1.8260 - val_accuracy: 0.3115\n",
      "Epoch 392/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8106 - accuracy: 0.3257 - val_loss: 1.8258 - val_accuracy: 0.3095\n",
      "Epoch 393/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8101 - accuracy: 0.3248 - val_loss: 1.8271 - val_accuracy: 0.3135\n",
      "Epoch 394/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8120 - accuracy: 0.3299 - val_loss: 1.8267 - val_accuracy: 0.3155\n",
      "Epoch 395/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8096 - accuracy: 0.3240 - val_loss: 1.8243 - val_accuracy: 0.3135\n",
      "Epoch 396/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8096 - accuracy: 0.3240 - val_loss: 1.8267 - val_accuracy: 0.3095\n",
      "Epoch 397/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8100 - accuracy: 0.3274 - val_loss: 1.8257 - val_accuracy: 0.3135\n",
      "Epoch 398/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8113 - accuracy: 0.3282 - val_loss: 1.8243 - val_accuracy: 0.3115\n",
      "Epoch 399/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8100 - accuracy: 0.3155 - val_loss: 1.8242 - val_accuracy: 0.3135\n",
      "Epoch 400/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8091 - accuracy: 0.3197 - val_loss: 1.8254 - val_accuracy: 0.3175\n",
      "Epoch 401/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8089 - accuracy: 0.3248 - val_loss: 1.8247 - val_accuracy: 0.3175\n",
      "Epoch 402/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8098 - accuracy: 0.3299 - val_loss: 1.8244 - val_accuracy: 0.3115\n",
      "Epoch 403/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8093 - accuracy: 0.3240 - val_loss: 1.8285 - val_accuracy: 0.3155\n",
      "Epoch 404/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8089 - accuracy: 0.3282 - val_loss: 1.8266 - val_accuracy: 0.3135\n",
      "Epoch 405/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8090 - accuracy: 0.3282 - val_loss: 1.8239 - val_accuracy: 0.3075\n",
      "Epoch 406/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8087 - accuracy: 0.3240 - val_loss: 1.8259 - val_accuracy: 0.3155\n",
      "Epoch 407/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8083 - accuracy: 0.3265 - val_loss: 1.8231 - val_accuracy: 0.3095\n",
      "Epoch 408/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8077 - accuracy: 0.3291 - val_loss: 1.8231 - val_accuracy: 0.3095\n",
      "Epoch 409/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8069 - accuracy: 0.3282 - val_loss: 1.8238 - val_accuracy: 0.3095\n",
      "Epoch 410/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8092 - accuracy: 0.3240 - val_loss: 1.8232 - val_accuracy: 0.3115\n",
      "Epoch 411/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8072 - accuracy: 0.3299 - val_loss: 1.8230 - val_accuracy: 0.3135\n",
      "Epoch 412/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8078 - accuracy: 0.3308 - val_loss: 1.8231 - val_accuracy: 0.3135\n",
      "Epoch 413/700\n",
      "1176/1176 [==============================] - 0s 75us/step - loss: 1.8081 - accuracy: 0.3240 - val_loss: 1.8238 - val_accuracy: 0.3155\n",
      "Epoch 414/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8075 - accuracy: 0.3231 - val_loss: 1.8246 - val_accuracy: 0.3115\n",
      "Epoch 415/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8069 - accuracy: 0.3248 - val_loss: 1.8222 - val_accuracy: 0.3155\n",
      "Epoch 416/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8076 - accuracy: 0.3291 - val_loss: 1.8225 - val_accuracy: 0.3155\n",
      "Epoch 417/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8073 - accuracy: 0.3299 - val_loss: 1.8226 - val_accuracy: 0.3135\n",
      "Epoch 418/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8079 - accuracy: 0.3231 - val_loss: 1.8226 - val_accuracy: 0.3175\n",
      "Epoch 419/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8069 - accuracy: 0.3282 - val_loss: 1.8223 - val_accuracy: 0.3155\n",
      "Epoch 420/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8075 - accuracy: 0.3274 - val_loss: 1.8220 - val_accuracy: 0.3115\n",
      "Epoch 421/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8064 - accuracy: 0.3240 - val_loss: 1.8236 - val_accuracy: 0.3135\n",
      "Epoch 422/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8073 - accuracy: 0.3274 - val_loss: 1.8263 - val_accuracy: 0.3175\n",
      "Epoch 423/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8064 - accuracy: 0.3265 - val_loss: 1.8232 - val_accuracy: 0.3135\n",
      "Epoch 424/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8062 - accuracy: 0.3265 - val_loss: 1.8234 - val_accuracy: 0.3135\n",
      "Epoch 425/700\n",
      "1176/1176 [==============================] - 0s 78us/step - loss: 1.8061 - accuracy: 0.3231 - val_loss: 1.8231 - val_accuracy: 0.3135\n",
      "Epoch 426/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8055 - accuracy: 0.3223 - val_loss: 1.8208 - val_accuracy: 0.3135\n",
      "Epoch 427/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8063 - accuracy: 0.3223 - val_loss: 1.8229 - val_accuracy: 0.3135\n",
      "Epoch 428/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8066 - accuracy: 0.3240 - val_loss: 1.8224 - val_accuracy: 0.3115\n",
      "Epoch 429/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8057 - accuracy: 0.3240 - val_loss: 1.8214 - val_accuracy: 0.3115\n",
      "Epoch 430/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8054 - accuracy: 0.3265 - val_loss: 1.8203 - val_accuracy: 0.3155\n",
      "Epoch 431/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8053 - accuracy: 0.3248 - val_loss: 1.8205 - val_accuracy: 0.3095\n",
      "Epoch 432/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8055 - accuracy: 0.3214 - val_loss: 1.8216 - val_accuracy: 0.3175\n",
      "Epoch 433/700\n",
      "1176/1176 [==============================] - 0s 85us/step - loss: 1.8050 - accuracy: 0.3282 - val_loss: 1.8222 - val_accuracy: 0.3175\n",
      "Epoch 434/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8053 - accuracy: 0.3223 - val_loss: 1.8207 - val_accuracy: 0.3115\n",
      "Epoch 435/700\n",
      "1176/1176 [==============================] - 0s 77us/step - loss: 1.8054 - accuracy: 0.3291 - val_loss: 1.8218 - val_accuracy: 0.3135\n",
      "Epoch 436/700\n",
      "1176/1176 [==============================] - 0s 76us/step - loss: 1.8049 - accuracy: 0.3248 - val_loss: 1.8235 - val_accuracy: 0.3155\n",
      "Epoch 437/700\n",
      "1176/1176 [==============================] - 0s 89us/step - loss: 1.8053 - accuracy: 0.3223 - val_loss: 1.8249 - val_accuracy: 0.3155\n",
      "Epoch 438/700\n",
      "1176/1176 [==============================] - 0s 81us/step - loss: 1.8037 - accuracy: 0.3223 - val_loss: 1.8205 - val_accuracy: 0.3155\n",
      "Epoch 439/700\n",
      "1176/1176 [==============================] - 0s 76us/step - loss: 1.8058 - accuracy: 0.3189 - val_loss: 1.8204 - val_accuracy: 0.3135\n",
      "Epoch 440/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.8055 - accuracy: 0.3223 - val_loss: 1.8198 - val_accuracy: 0.3115\n",
      "Epoch 441/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8041 - accuracy: 0.3257 - val_loss: 1.8227 - val_accuracy: 0.3135\n",
      "Epoch 442/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8037 - accuracy: 0.3265 - val_loss: 1.8192 - val_accuracy: 0.3194\n",
      "Epoch 443/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8036 - accuracy: 0.3282 - val_loss: 1.8220 - val_accuracy: 0.3155\n",
      "Epoch 444/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8042 - accuracy: 0.3231 - val_loss: 1.8202 - val_accuracy: 0.3155\n",
      "Epoch 445/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8038 - accuracy: 0.3248 - val_loss: 1.8226 - val_accuracy: 0.3135\n",
      "Epoch 446/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8050 - accuracy: 0.3214 - val_loss: 1.8221 - val_accuracy: 0.3155\n",
      "Epoch 447/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8037 - accuracy: 0.3248 - val_loss: 1.8192 - val_accuracy: 0.3194\n",
      "Epoch 448/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8038 - accuracy: 0.3248 - val_loss: 1.8207 - val_accuracy: 0.3155\n",
      "Epoch 449/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8044 - accuracy: 0.3214 - val_loss: 1.8197 - val_accuracy: 0.3155\n",
      "Epoch 450/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8028 - accuracy: 0.3291 - val_loss: 1.8215 - val_accuracy: 0.3135\n",
      "Epoch 451/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8031 - accuracy: 0.3231 - val_loss: 1.8202 - val_accuracy: 0.3135\n",
      "Epoch 452/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8031 - accuracy: 0.3265 - val_loss: 1.8201 - val_accuracy: 0.3155\n",
      "Epoch 453/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8033 - accuracy: 0.3240 - val_loss: 1.8213 - val_accuracy: 0.3135\n",
      "Epoch 454/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8031 - accuracy: 0.3257 - val_loss: 1.8206 - val_accuracy: 0.3155\n",
      "Epoch 455/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8026 - accuracy: 0.3282 - val_loss: 1.8237 - val_accuracy: 0.3155\n",
      "Epoch 456/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8029 - accuracy: 0.3282 - val_loss: 1.8201 - val_accuracy: 0.3194\n",
      "Epoch 457/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8025 - accuracy: 0.3240 - val_loss: 1.8220 - val_accuracy: 0.3155\n",
      "Epoch 458/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8036 - accuracy: 0.3240 - val_loss: 1.8191 - val_accuracy: 0.3115\n",
      "Epoch 459/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8028 - accuracy: 0.3299 - val_loss: 1.8175 - val_accuracy: 0.3155\n",
      "Epoch 460/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8024 - accuracy: 0.3223 - val_loss: 1.8179 - val_accuracy: 0.3095\n",
      "Epoch 461/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8026 - accuracy: 0.3265 - val_loss: 1.8187 - val_accuracy: 0.3194\n",
      "Epoch 462/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8026 - accuracy: 0.3274 - val_loss: 1.8198 - val_accuracy: 0.3135\n",
      "Epoch 463/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8026 - accuracy: 0.3274 - val_loss: 1.8176 - val_accuracy: 0.3095\n",
      "Epoch 464/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8025 - accuracy: 0.3231 - val_loss: 1.8227 - val_accuracy: 0.3175\n",
      "Epoch 465/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8037 - accuracy: 0.3197 - val_loss: 1.8221 - val_accuracy: 0.3175\n",
      "Epoch 466/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8021 - accuracy: 0.3248 - val_loss: 1.8210 - val_accuracy: 0.3155\n",
      "Epoch 467/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.8027 - accuracy: 0.3291 - val_loss: 1.8183 - val_accuracy: 0.3194\n",
      "Epoch 468/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8024 - accuracy: 0.3257 - val_loss: 1.8197 - val_accuracy: 0.3135\n",
      "Epoch 469/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8028 - accuracy: 0.3240 - val_loss: 1.8186 - val_accuracy: 0.3214\n",
      "Epoch 470/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8025 - accuracy: 0.3265 - val_loss: 1.8189 - val_accuracy: 0.3175\n",
      "Epoch 471/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8021 - accuracy: 0.3197 - val_loss: 1.8185 - val_accuracy: 0.3175\n",
      "Epoch 472/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8016 - accuracy: 0.3274 - val_loss: 1.8223 - val_accuracy: 0.3194\n",
      "Epoch 473/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8008 - accuracy: 0.3291 - val_loss: 1.8178 - val_accuracy: 0.3075\n",
      "Epoch 474/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8022 - accuracy: 0.3206 - val_loss: 1.8185 - val_accuracy: 0.3155\n",
      "Epoch 475/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8019 - accuracy: 0.3248 - val_loss: 1.8183 - val_accuracy: 0.3155\n",
      "Epoch 476/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8008 - accuracy: 0.3257 - val_loss: 1.8243 - val_accuracy: 0.3175\n",
      "Epoch 477/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8018 - accuracy: 0.3231 - val_loss: 1.8186 - val_accuracy: 0.3155\n",
      "Epoch 478/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8013 - accuracy: 0.3240 - val_loss: 1.8186 - val_accuracy: 0.3155\n",
      "Epoch 479/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8018 - accuracy: 0.3248 - val_loss: 1.8202 - val_accuracy: 0.3214\n",
      "Epoch 480/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8013 - accuracy: 0.3223 - val_loss: 1.8170 - val_accuracy: 0.3175\n",
      "Epoch 481/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8010 - accuracy: 0.3274 - val_loss: 1.8181 - val_accuracy: 0.3214\n",
      "Epoch 482/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8009 - accuracy: 0.3223 - val_loss: 1.8197 - val_accuracy: 0.3194\n",
      "Epoch 483/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.8004 - accuracy: 0.3248 - val_loss: 1.8183 - val_accuracy: 0.3194\n",
      "Epoch 484/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8009 - accuracy: 0.3274 - val_loss: 1.8202 - val_accuracy: 0.3194\n",
      "Epoch 485/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8006 - accuracy: 0.3257 - val_loss: 1.8211 - val_accuracy: 0.3214\n",
      "Epoch 486/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8014 - accuracy: 0.3223 - val_loss: 1.8214 - val_accuracy: 0.3175\n",
      "Epoch 487/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.8013 - accuracy: 0.3214 - val_loss: 1.8194 - val_accuracy: 0.3175\n",
      "Epoch 488/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8009 - accuracy: 0.3248 - val_loss: 1.8169 - val_accuracy: 0.3194\n",
      "Epoch 489/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.8009 - accuracy: 0.3223 - val_loss: 1.8202 - val_accuracy: 0.3194\n",
      "Epoch 490/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8007 - accuracy: 0.3214 - val_loss: 1.8173 - val_accuracy: 0.3214\n",
      "Epoch 491/700\n",
      "1176/1176 [==============================] - 0s 73us/step - loss: 1.8009 - accuracy: 0.3265 - val_loss: 1.8175 - val_accuracy: 0.3175\n",
      "Epoch 492/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.8001 - accuracy: 0.3282 - val_loss: 1.8159 - val_accuracy: 0.3155\n",
      "Epoch 493/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.8007 - accuracy: 0.3257 - val_loss: 1.8156 - val_accuracy: 0.3135\n",
      "Epoch 494/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8004 - accuracy: 0.3248 - val_loss: 1.8170 - val_accuracy: 0.3175\n",
      "Epoch 495/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.8001 - accuracy: 0.3274 - val_loss: 1.8157 - val_accuracy: 0.3175\n",
      "Epoch 496/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7996 - accuracy: 0.3240 - val_loss: 1.8162 - val_accuracy: 0.3135\n",
      "Epoch 497/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7999 - accuracy: 0.3257 - val_loss: 1.8193 - val_accuracy: 0.3175\n",
      "Epoch 498/700\n",
      "1176/1176 [==============================] - 0s 82us/step - loss: 1.7998 - accuracy: 0.3248 - val_loss: 1.8173 - val_accuracy: 0.3194\n",
      "Epoch 499/700\n",
      "1176/1176 [==============================] - 0s 79us/step - loss: 1.7998 - accuracy: 0.3308 - val_loss: 1.8159 - val_accuracy: 0.3135\n",
      "Epoch 500/700\n",
      "1176/1176 [==============================] - 0s 91us/step - loss: 1.7995 - accuracy: 0.3299 - val_loss: 1.8158 - val_accuracy: 0.3095\n",
      "Epoch 501/700\n",
      "1176/1176 [==============================] - 0s 80us/step - loss: 1.7998 - accuracy: 0.3282 - val_loss: 1.8170 - val_accuracy: 0.3155\n",
      "Epoch 502/700\n",
      "1176/1176 [==============================] - 0s 76us/step - loss: 1.8003 - accuracy: 0.3274 - val_loss: 1.8158 - val_accuracy: 0.3095\n",
      "Epoch 503/700\n",
      "1176/1176 [==============================] - 0s 75us/step - loss: 1.7996 - accuracy: 0.3240 - val_loss: 1.8150 - val_accuracy: 0.3115\n",
      "Epoch 504/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7991 - accuracy: 0.3291 - val_loss: 1.8195 - val_accuracy: 0.3214\n",
      "Epoch 505/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7993 - accuracy: 0.3248 - val_loss: 1.8167 - val_accuracy: 0.3175\n",
      "Epoch 506/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.7993 - accuracy: 0.3265 - val_loss: 1.8167 - val_accuracy: 0.3194\n",
      "Epoch 507/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7993 - accuracy: 0.3257 - val_loss: 1.8160 - val_accuracy: 0.3155\n",
      "Epoch 508/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7996 - accuracy: 0.3257 - val_loss: 1.8180 - val_accuracy: 0.3175\n",
      "Epoch 509/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7995 - accuracy: 0.3308 - val_loss: 1.8150 - val_accuracy: 0.3115\n",
      "Epoch 510/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7990 - accuracy: 0.3282 - val_loss: 1.8153 - val_accuracy: 0.3115\n",
      "Epoch 511/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7995 - accuracy: 0.3240 - val_loss: 1.8176 - val_accuracy: 0.3155\n",
      "Epoch 512/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.7993 - accuracy: 0.3257 - val_loss: 1.8148 - val_accuracy: 0.3214\n",
      "Epoch 513/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7985 - accuracy: 0.3274 - val_loss: 1.8180 - val_accuracy: 0.3194\n",
      "Epoch 514/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.7986 - accuracy: 0.3291 - val_loss: 1.8144 - val_accuracy: 0.3155\n",
      "Epoch 515/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7993 - accuracy: 0.3308 - val_loss: 1.8147 - val_accuracy: 0.3194\n",
      "Epoch 516/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7981 - accuracy: 0.3274 - val_loss: 1.8207 - val_accuracy: 0.3234\n",
      "Epoch 517/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.7991 - accuracy: 0.3291 - val_loss: 1.8168 - val_accuracy: 0.3194\n",
      "Epoch 518/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7984 - accuracy: 0.3291 - val_loss: 1.8179 - val_accuracy: 0.3155\n",
      "Epoch 519/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7990 - accuracy: 0.3308 - val_loss: 1.8177 - val_accuracy: 0.3175\n",
      "Epoch 520/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.7983 - accuracy: 0.3291 - val_loss: 1.8179 - val_accuracy: 0.3155\n",
      "Epoch 521/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7995 - accuracy: 0.3257 - val_loss: 1.8146 - val_accuracy: 0.3175\n",
      "Epoch 522/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7985 - accuracy: 0.3265 - val_loss: 1.8168 - val_accuracy: 0.3234\n",
      "Epoch 523/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7983 - accuracy: 0.3265 - val_loss: 1.8168 - val_accuracy: 0.3274\n",
      "Epoch 524/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7978 - accuracy: 0.3282 - val_loss: 1.8143 - val_accuracy: 0.3135\n",
      "Epoch 525/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7989 - accuracy: 0.3265 - val_loss: 1.8154 - val_accuracy: 0.3155\n",
      "Epoch 526/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7981 - accuracy: 0.3248 - val_loss: 1.8153 - val_accuracy: 0.3155\n",
      "Epoch 527/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7974 - accuracy: 0.3265 - val_loss: 1.8189 - val_accuracy: 0.3194\n",
      "Epoch 528/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.7978 - accuracy: 0.3172 - val_loss: 1.8186 - val_accuracy: 0.3214\n",
      "Epoch 529/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7988 - accuracy: 0.3240 - val_loss: 1.8163 - val_accuracy: 0.3214\n",
      "Epoch 530/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7983 - accuracy: 0.3231 - val_loss: 1.8143 - val_accuracy: 0.3234\n",
      "Epoch 531/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.7971 - accuracy: 0.3265 - val_loss: 1.8131 - val_accuracy: 0.3155\n",
      "Epoch 532/700\n",
      "1176/1176 [==============================] - 0s 73us/step - loss: 1.7975 - accuracy: 0.3240 - val_loss: 1.8156 - val_accuracy: 0.3274\n",
      "Epoch 533/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.7978 - accuracy: 0.3240 - val_loss: 1.8155 - val_accuracy: 0.3234\n",
      "Epoch 534/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7975 - accuracy: 0.3240 - val_loss: 1.8138 - val_accuracy: 0.3155\n",
      "Epoch 535/700\n",
      "1176/1176 [==============================] - 0s 78us/step - loss: 1.7973 - accuracy: 0.3231 - val_loss: 1.8138 - val_accuracy: 0.3115\n",
      "Epoch 536/700\n",
      "1176/1176 [==============================] - 0s 85us/step - loss: 1.7980 - accuracy: 0.3265 - val_loss: 1.8139 - val_accuracy: 0.3135\n",
      "Epoch 537/700\n",
      "1176/1176 [==============================] - 0s 87us/step - loss: 1.7972 - accuracy: 0.3257 - val_loss: 1.8145 - val_accuracy: 0.3135\n",
      "Epoch 538/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7979 - accuracy: 0.3223 - val_loss: 1.8161 - val_accuracy: 0.3214\n",
      "Epoch 539/700\n",
      "1176/1176 [==============================] - 0s 95us/step - loss: 1.7982 - accuracy: 0.3248 - val_loss: 1.8163 - val_accuracy: 0.3214\n",
      "Epoch 540/700\n",
      "1176/1176 [==============================] - 0s 93us/step - loss: 1.7970 - accuracy: 0.3240 - val_loss: 1.8171 - val_accuracy: 0.3194\n",
      "Epoch 541/700\n",
      "1176/1176 [==============================] - 0s 76us/step - loss: 1.7972 - accuracy: 0.3274 - val_loss: 1.8160 - val_accuracy: 0.3214\n",
      "Epoch 542/700\n",
      "1176/1176 [==============================] - 0s 87us/step - loss: 1.7972 - accuracy: 0.3274 - val_loss: 1.8141 - val_accuracy: 0.3175\n",
      "Epoch 543/700\n",
      "1176/1176 [==============================] - 0s 96us/step - loss: 1.7973 - accuracy: 0.3291 - val_loss: 1.8132 - val_accuracy: 0.3175\n",
      "Epoch 544/700\n",
      "1176/1176 [==============================] - 0s 81us/step - loss: 1.7974 - accuracy: 0.3223 - val_loss: 1.8133 - val_accuracy: 0.3175\n",
      "Epoch 545/700\n",
      "1176/1176 [==============================] - 0s 77us/step - loss: 1.7974 - accuracy: 0.3265 - val_loss: 1.8158 - val_accuracy: 0.3234\n",
      "Epoch 546/700\n",
      "1176/1176 [==============================] - 0s 79us/step - loss: 1.7977 - accuracy: 0.3274 - val_loss: 1.8159 - val_accuracy: 0.3274\n",
      "Epoch 547/700\n",
      "1176/1176 [==============================] - 0s 77us/step - loss: 1.7974 - accuracy: 0.3248 - val_loss: 1.8162 - val_accuracy: 0.3234\n",
      "Epoch 548/700\n",
      "1176/1176 [==============================] - 0s 82us/step - loss: 1.7967 - accuracy: 0.3265 - val_loss: 1.8157 - val_accuracy: 0.3234\n",
      "Epoch 549/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7972 - accuracy: 0.3248 - val_loss: 1.8136 - val_accuracy: 0.3175\n",
      "Epoch 550/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7970 - accuracy: 0.3214 - val_loss: 1.8148 - val_accuracy: 0.3214\n",
      "Epoch 551/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7960 - accuracy: 0.3291 - val_loss: 1.8206 - val_accuracy: 0.3234\n",
      "Epoch 552/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 83us/step - loss: 1.7973 - accuracy: 0.3282 - val_loss: 1.8186 - val_accuracy: 0.3194\n",
      "Epoch 553/700\n",
      "1176/1176 [==============================] - 0s 83us/step - loss: 1.7969 - accuracy: 0.3248 - val_loss: 1.8134 - val_accuracy: 0.3194\n",
      "Epoch 554/700\n",
      "1176/1176 [==============================] - 0s 87us/step - loss: 1.7968 - accuracy: 0.3274 - val_loss: 1.8131 - val_accuracy: 0.3194\n",
      "Epoch 555/700\n",
      "1176/1176 [==============================] - 0s 76us/step - loss: 1.7968 - accuracy: 0.3274 - val_loss: 1.8167 - val_accuracy: 0.3234\n",
      "Epoch 556/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7959 - accuracy: 0.3257 - val_loss: 1.8122 - val_accuracy: 0.3175\n",
      "Epoch 557/700\n",
      "1176/1176 [==============================] - 0s 73us/step - loss: 1.7972 - accuracy: 0.3274 - val_loss: 1.8130 - val_accuracy: 0.3194\n",
      "Epoch 558/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.7963 - accuracy: 0.3265 - val_loss: 1.8145 - val_accuracy: 0.3194\n",
      "Epoch 559/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7960 - accuracy: 0.3257 - val_loss: 1.8128 - val_accuracy: 0.3135\n",
      "Epoch 560/700\n",
      "1176/1176 [==============================] - 0s 76us/step - loss: 1.7966 - accuracy: 0.3274 - val_loss: 1.8142 - val_accuracy: 0.3135\n",
      "Epoch 561/700\n",
      "1176/1176 [==============================] - 0s 77us/step - loss: 1.7967 - accuracy: 0.3265 - val_loss: 1.8163 - val_accuracy: 0.3194\n",
      "Epoch 562/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7964 - accuracy: 0.3257 - val_loss: 1.8152 - val_accuracy: 0.3234\n",
      "Epoch 563/700\n",
      "1176/1176 [==============================] - 0s 80us/step - loss: 1.7965 - accuracy: 0.3265 - val_loss: 1.8177 - val_accuracy: 0.3234\n",
      "Epoch 564/700\n",
      "1176/1176 [==============================] - 0s 80us/step - loss: 1.7968 - accuracy: 0.3248 - val_loss: 1.8144 - val_accuracy: 0.3135\n",
      "Epoch 565/700\n",
      "1176/1176 [==============================] - 0s 79us/step - loss: 1.7963 - accuracy: 0.3274 - val_loss: 1.8161 - val_accuracy: 0.3175\n",
      "Epoch 566/700\n",
      "1176/1176 [==============================] - 0s 79us/step - loss: 1.7965 - accuracy: 0.3257 - val_loss: 1.8171 - val_accuracy: 0.3214\n",
      "Epoch 567/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.7970 - accuracy: 0.3282 - val_loss: 1.8145 - val_accuracy: 0.3155\n",
      "Epoch 568/700\n",
      "1176/1176 [==============================] - 0s 75us/step - loss: 1.7965 - accuracy: 0.3248 - val_loss: 1.8159 - val_accuracy: 0.3175\n",
      "Epoch 569/700\n",
      "1176/1176 [==============================] - 0s 80us/step - loss: 1.7955 - accuracy: 0.3291 - val_loss: 1.8122 - val_accuracy: 0.3135\n",
      "Epoch 570/700\n",
      "1176/1176 [==============================] - 0s 82us/step - loss: 1.7957 - accuracy: 0.3299 - val_loss: 1.8155 - val_accuracy: 0.3194\n",
      "Epoch 571/700\n",
      "1176/1176 [==============================] - 0s 78us/step - loss: 1.7966 - accuracy: 0.3282 - val_loss: 1.8144 - val_accuracy: 0.3194\n",
      "Epoch 572/700\n",
      "1176/1176 [==============================] - 0s 79us/step - loss: 1.7960 - accuracy: 0.3274 - val_loss: 1.8127 - val_accuracy: 0.3175\n",
      "Epoch 573/700\n",
      "1176/1176 [==============================] - 0s 99us/step - loss: 1.7959 - accuracy: 0.3257 - val_loss: 1.8142 - val_accuracy: 0.3194\n",
      "Epoch 574/700\n",
      "1176/1176 [==============================] - 0s 82us/step - loss: 1.7958 - accuracy: 0.3274 - val_loss: 1.8126 - val_accuracy: 0.3234\n",
      "Epoch 575/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.7960 - accuracy: 0.3274 - val_loss: 1.8152 - val_accuracy: 0.3234\n",
      "Epoch 576/700\n",
      "1176/1176 [==============================] - 0s 90us/step - loss: 1.7960 - accuracy: 0.3282 - val_loss: 1.8151 - val_accuracy: 0.3175\n",
      "Epoch 577/700\n",
      "1176/1176 [==============================] - 0s 82us/step - loss: 1.7956 - accuracy: 0.3265 - val_loss: 1.8152 - val_accuracy: 0.3175\n",
      "Epoch 578/700\n",
      "1176/1176 [==============================] - 0s 78us/step - loss: 1.7962 - accuracy: 0.3265 - val_loss: 1.8136 - val_accuracy: 0.3214\n",
      "Epoch 579/700\n",
      "1176/1176 [==============================] - 0s 95us/step - loss: 1.7954 - accuracy: 0.3291 - val_loss: 1.8129 - val_accuracy: 0.3194\n",
      "Epoch 580/700\n",
      "1176/1176 [==============================] - 0s 81us/step - loss: 1.7952 - accuracy: 0.3274 - val_loss: 1.8155 - val_accuracy: 0.3214\n",
      "Epoch 581/700\n",
      "1176/1176 [==============================] - 0s 83us/step - loss: 1.7960 - accuracy: 0.3274 - val_loss: 1.8128 - val_accuracy: 0.3155\n",
      "Epoch 582/700\n",
      "1176/1176 [==============================] - 0s 81us/step - loss: 1.7955 - accuracy: 0.3231 - val_loss: 1.8165 - val_accuracy: 0.3234\n",
      "Epoch 583/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7957 - accuracy: 0.3265 - val_loss: 1.8125 - val_accuracy: 0.3175\n",
      "Epoch 584/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7948 - accuracy: 0.3223 - val_loss: 1.8117 - val_accuracy: 0.3175\n",
      "Epoch 585/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7952 - accuracy: 0.3257 - val_loss: 1.8150 - val_accuracy: 0.3194\n",
      "Epoch 586/700\n",
      "1176/1176 [==============================] - 0s 82us/step - loss: 1.7956 - accuracy: 0.3265 - val_loss: 1.8167 - val_accuracy: 0.3254\n",
      "Epoch 587/700\n",
      "1176/1176 [==============================] - 0s 75us/step - loss: 1.7957 - accuracy: 0.3291 - val_loss: 1.8131 - val_accuracy: 0.3175\n",
      "Epoch 588/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7954 - accuracy: 0.3316 - val_loss: 1.8139 - val_accuracy: 0.3135\n",
      "Epoch 589/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7951 - accuracy: 0.3257 - val_loss: 1.8127 - val_accuracy: 0.3194\n",
      "Epoch 590/700\n",
      "1176/1176 [==============================] - 0s 83us/step - loss: 1.7954 - accuracy: 0.3282 - val_loss: 1.8129 - val_accuracy: 0.3135\n",
      "Epoch 591/700\n",
      "1176/1176 [==============================] - 0s 75us/step - loss: 1.7958 - accuracy: 0.3265 - val_loss: 1.8128 - val_accuracy: 0.3135\n",
      "Epoch 592/700\n",
      "1176/1176 [==============================] - 0s 78us/step - loss: 1.7949 - accuracy: 0.3265 - val_loss: 1.8150 - val_accuracy: 0.3214\n",
      "Epoch 593/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7947 - accuracy: 0.3274 - val_loss: 1.8112 - val_accuracy: 0.3155\n",
      "Epoch 594/700\n",
      "1176/1176 [==============================] - 0s 78us/step - loss: 1.7946 - accuracy: 0.3274 - val_loss: 1.8111 - val_accuracy: 0.3115\n",
      "Epoch 595/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7955 - accuracy: 0.3248 - val_loss: 1.8123 - val_accuracy: 0.3175\n",
      "Epoch 596/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7946 - accuracy: 0.3299 - val_loss: 1.8132 - val_accuracy: 0.3175\n",
      "Epoch 597/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7947 - accuracy: 0.3257 - val_loss: 1.8138 - val_accuracy: 0.3254\n",
      "Epoch 598/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7947 - accuracy: 0.3308 - val_loss: 1.8138 - val_accuracy: 0.3214\n",
      "Epoch 599/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7944 - accuracy: 0.3248 - val_loss: 1.8115 - val_accuracy: 0.3135\n",
      "Epoch 600/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7948 - accuracy: 0.3274 - val_loss: 1.8112 - val_accuracy: 0.3135\n",
      "Epoch 601/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7948 - accuracy: 0.3274 - val_loss: 1.8130 - val_accuracy: 0.3214\n",
      "Epoch 602/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7943 - accuracy: 0.3265 - val_loss: 1.8113 - val_accuracy: 0.3155\n",
      "Epoch 603/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7953 - accuracy: 0.3257 - val_loss: 1.8126 - val_accuracy: 0.3175\n",
      "Epoch 604/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7947 - accuracy: 0.3274 - val_loss: 1.8131 - val_accuracy: 0.3175\n",
      "Epoch 605/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7945 - accuracy: 0.3291 - val_loss: 1.8135 - val_accuracy: 0.3175\n",
      "Epoch 606/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7951 - accuracy: 0.3240 - val_loss: 1.8131 - val_accuracy: 0.3194\n",
      "Epoch 607/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7952 - accuracy: 0.3274 - val_loss: 1.8131 - val_accuracy: 0.3135\n",
      "Epoch 608/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7955 - accuracy: 0.3240 - val_loss: 1.8129 - val_accuracy: 0.3175\n",
      "Epoch 609/700\n",
      "1176/1176 [==============================] - 0s 80us/step - loss: 1.7949 - accuracy: 0.3231 - val_loss: 1.8137 - val_accuracy: 0.3155\n",
      "Epoch 610/700\n",
      "1176/1176 [==============================] - 0s 77us/step - loss: 1.7942 - accuracy: 0.3231 - val_loss: 1.8129 - val_accuracy: 0.3155\n",
      "Epoch 611/700\n",
      "1176/1176 [==============================] - 0s 79us/step - loss: 1.7944 - accuracy: 0.3257 - val_loss: 1.8151 - val_accuracy: 0.3234\n",
      "Epoch 612/700\n",
      "1176/1176 [==============================] - 0s 73us/step - loss: 1.7946 - accuracy: 0.3248 - val_loss: 1.8121 - val_accuracy: 0.3234\n",
      "Epoch 613/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7946 - accuracy: 0.3265 - val_loss: 1.8128 - val_accuracy: 0.3194\n",
      "Epoch 614/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7944 - accuracy: 0.3257 - val_loss: 1.8106 - val_accuracy: 0.3214\n",
      "Epoch 615/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7951 - accuracy: 0.3265 - val_loss: 1.8108 - val_accuracy: 0.3155\n",
      "Epoch 616/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7938 - accuracy: 0.3265 - val_loss: 1.8107 - val_accuracy: 0.3155\n",
      "Epoch 617/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7934 - accuracy: 0.3257 - val_loss: 1.8145 - val_accuracy: 0.3194\n",
      "Epoch 618/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7941 - accuracy: 0.3265 - val_loss: 1.8133 - val_accuracy: 0.3135\n",
      "Epoch 619/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7938 - accuracy: 0.3231 - val_loss: 1.8133 - val_accuracy: 0.3194\n",
      "Epoch 620/700\n",
      "1176/1176 [==============================] - 0s 72us/step - loss: 1.7938 - accuracy: 0.3257 - val_loss: 1.8109 - val_accuracy: 0.3155\n",
      "Epoch 621/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7934 - accuracy: 0.3299 - val_loss: 1.8104 - val_accuracy: 0.3135\n",
      "Epoch 622/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7943 - accuracy: 0.3248 - val_loss: 1.8131 - val_accuracy: 0.3214\n",
      "Epoch 623/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7935 - accuracy: 0.3240 - val_loss: 1.8103 - val_accuracy: 0.3155\n",
      "Epoch 624/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7944 - accuracy: 0.3299 - val_loss: 1.8114 - val_accuracy: 0.3194\n",
      "Epoch 625/700\n",
      "1176/1176 [==============================] - 0s 74us/step - loss: 1.7946 - accuracy: 0.3231 - val_loss: 1.8113 - val_accuracy: 0.3194\n",
      "Epoch 626/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7935 - accuracy: 0.3265 - val_loss: 1.8107 - val_accuracy: 0.3155\n",
      "Epoch 627/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7938 - accuracy: 0.3325 - val_loss: 1.8141 - val_accuracy: 0.3194\n",
      "Epoch 628/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7938 - accuracy: 0.3274 - val_loss: 1.8115 - val_accuracy: 0.3155\n",
      "Epoch 629/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7936 - accuracy: 0.3282 - val_loss: 1.8113 - val_accuracy: 0.3175\n",
      "Epoch 630/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7933 - accuracy: 0.3282 - val_loss: 1.8098 - val_accuracy: 0.3135\n",
      "Epoch 631/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7939 - accuracy: 0.3308 - val_loss: 1.8099 - val_accuracy: 0.3135\n",
      "Epoch 632/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7931 - accuracy: 0.3291 - val_loss: 1.8099 - val_accuracy: 0.3155\n",
      "Epoch 633/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7934 - accuracy: 0.3248 - val_loss: 1.8120 - val_accuracy: 0.3214\n",
      "Epoch 634/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7937 - accuracy: 0.3265 - val_loss: 1.8116 - val_accuracy: 0.3214\n",
      "Epoch 635/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7937 - accuracy: 0.3248 - val_loss: 1.8132 - val_accuracy: 0.3234\n",
      "Epoch 636/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7935 - accuracy: 0.3282 - val_loss: 1.8125 - val_accuracy: 0.3214\n",
      "Epoch 637/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7933 - accuracy: 0.3282 - val_loss: 1.8114 - val_accuracy: 0.3234\n",
      "Epoch 638/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7932 - accuracy: 0.3291 - val_loss: 1.8102 - val_accuracy: 0.3115\n",
      "Epoch 639/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7934 - accuracy: 0.3299 - val_loss: 1.8121 - val_accuracy: 0.3234\n",
      "Epoch 640/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7935 - accuracy: 0.3282 - val_loss: 1.8093 - val_accuracy: 0.3194\n",
      "Epoch 641/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7934 - accuracy: 0.3316 - val_loss: 1.8108 - val_accuracy: 0.3194\n",
      "Epoch 642/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7934 - accuracy: 0.3316 - val_loss: 1.8111 - val_accuracy: 0.3214\n",
      "Epoch 643/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7931 - accuracy: 0.3291 - val_loss: 1.8113 - val_accuracy: 0.3175\n",
      "Epoch 644/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7928 - accuracy: 0.3223 - val_loss: 1.8130 - val_accuracy: 0.3135\n",
      "Epoch 645/700\n",
      "1176/1176 [==============================] - 0s 71us/step - loss: 1.7929 - accuracy: 0.3257 - val_loss: 1.8105 - val_accuracy: 0.3135\n",
      "Epoch 646/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7937 - accuracy: 0.3282 - val_loss: 1.8113 - val_accuracy: 0.3135\n",
      "Epoch 647/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7932 - accuracy: 0.3240 - val_loss: 1.8099 - val_accuracy: 0.3135\n",
      "Epoch 648/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7930 - accuracy: 0.3282 - val_loss: 1.8120 - val_accuracy: 0.3214\n",
      "Epoch 649/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7925 - accuracy: 0.3240 - val_loss: 1.8088 - val_accuracy: 0.3155\n",
      "Epoch 650/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7928 - accuracy: 0.3282 - val_loss: 1.8128 - val_accuracy: 0.3214\n",
      "Epoch 651/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7925 - accuracy: 0.3274 - val_loss: 1.8095 - val_accuracy: 0.3135\n",
      "Epoch 652/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7930 - accuracy: 0.3282 - val_loss: 1.8103 - val_accuracy: 0.3155\n",
      "Epoch 653/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7931 - accuracy: 0.3316 - val_loss: 1.8099 - val_accuracy: 0.3155\n",
      "Epoch 654/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7929 - accuracy: 0.3299 - val_loss: 1.8099 - val_accuracy: 0.3175\n",
      "Epoch 655/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7925 - accuracy: 0.3257 - val_loss: 1.8102 - val_accuracy: 0.3175\n",
      "Epoch 656/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7930 - accuracy: 0.3282 - val_loss: 1.8096 - val_accuracy: 0.3155\n",
      "Epoch 657/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7927 - accuracy: 0.3274 - val_loss: 1.8094 - val_accuracy: 0.3194\n",
      "Epoch 658/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7930 - accuracy: 0.3308 - val_loss: 1.8101 - val_accuracy: 0.3155\n",
      "Epoch 659/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7918 - accuracy: 0.3325 - val_loss: 1.8135 - val_accuracy: 0.3214\n",
      "Epoch 660/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7928 - accuracy: 0.3257 - val_loss: 1.8136 - val_accuracy: 0.3214\n",
      "Epoch 661/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7930 - accuracy: 0.3299 - val_loss: 1.8113 - val_accuracy: 0.3194\n",
      "Epoch 662/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7926 - accuracy: 0.3282 - val_loss: 1.8107 - val_accuracy: 0.3234\n",
      "Epoch 663/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7926 - accuracy: 0.3257 - val_loss: 1.8124 - val_accuracy: 0.3155\n",
      "Epoch 664/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7926 - accuracy: 0.3257 - val_loss: 1.8095 - val_accuracy: 0.3214\n",
      "Epoch 665/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7926 - accuracy: 0.3248 - val_loss: 1.8122 - val_accuracy: 0.3214\n",
      "Epoch 666/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7922 - accuracy: 0.3274 - val_loss: 1.8114 - val_accuracy: 0.3214\n",
      "Epoch 667/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7924 - accuracy: 0.3274 - val_loss: 1.8097 - val_accuracy: 0.3155\n",
      "Epoch 668/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7925 - accuracy: 0.3265 - val_loss: 1.8093 - val_accuracy: 0.3175\n",
      "Epoch 669/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7924 - accuracy: 0.3274 - val_loss: 1.8099 - val_accuracy: 0.3135\n",
      "Epoch 670/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7920 - accuracy: 0.3291 - val_loss: 1.8095 - val_accuracy: 0.3075\n",
      "Epoch 671/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7922 - accuracy: 0.3274 - val_loss: 1.8100 - val_accuracy: 0.3175\n",
      "Epoch 672/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7922 - accuracy: 0.3265 - val_loss: 1.8100 - val_accuracy: 0.3115\n",
      "Epoch 673/700\n",
      "1176/1176 [==============================] - 0s 65us/step - loss: 1.7923 - accuracy: 0.3282 - val_loss: 1.8111 - val_accuracy: 0.3115\n",
      "Epoch 674/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7925 - accuracy: 0.3265 - val_loss: 1.8110 - val_accuracy: 0.3115\n",
      "Epoch 675/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7920 - accuracy: 0.3257 - val_loss: 1.8112 - val_accuracy: 0.3194\n",
      "Epoch 676/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7920 - accuracy: 0.3257 - val_loss: 1.8124 - val_accuracy: 0.3175\n",
      "Epoch 677/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7920 - accuracy: 0.3274 - val_loss: 1.8128 - val_accuracy: 0.3194\n",
      "Epoch 678/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7923 - accuracy: 0.3291 - val_loss: 1.8121 - val_accuracy: 0.3194\n",
      "Epoch 679/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7925 - accuracy: 0.3265 - val_loss: 1.8099 - val_accuracy: 0.3194\n",
      "Epoch 680/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7916 - accuracy: 0.3231 - val_loss: 1.8083 - val_accuracy: 0.3155\n",
      "Epoch 681/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7923 - accuracy: 0.3282 - val_loss: 1.8092 - val_accuracy: 0.3135\n",
      "Epoch 682/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7918 - accuracy: 0.3231 - val_loss: 1.8099 - val_accuracy: 0.3175\n",
      "Epoch 683/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7916 - accuracy: 0.3282 - val_loss: 1.8102 - val_accuracy: 0.3214\n",
      "Epoch 684/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7918 - accuracy: 0.3282 - val_loss: 1.8093 - val_accuracy: 0.3194\n",
      "Epoch 685/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7914 - accuracy: 0.3265 - val_loss: 1.8091 - val_accuracy: 0.3214\n",
      "Epoch 686/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7921 - accuracy: 0.3274 - val_loss: 1.8094 - val_accuracy: 0.3175\n",
      "Epoch 687/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7913 - accuracy: 0.3274 - val_loss: 1.8074 - val_accuracy: 0.3175\n",
      "Epoch 688/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7912 - accuracy: 0.3325 - val_loss: 1.8078 - val_accuracy: 0.3175\n",
      "Epoch 689/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7915 - accuracy: 0.3299 - val_loss: 1.8088 - val_accuracy: 0.3135\n",
      "Epoch 690/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7930 - accuracy: 0.3291 - val_loss: 1.8092 - val_accuracy: 0.3214\n",
      "Epoch 691/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7912 - accuracy: 0.3325 - val_loss: 1.8122 - val_accuracy: 0.3214\n",
      "Epoch 692/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7913 - accuracy: 0.3291 - val_loss: 1.8117 - val_accuracy: 0.3194\n",
      "Epoch 693/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7914 - accuracy: 0.3240 - val_loss: 1.8124 - val_accuracy: 0.3214\n",
      "Epoch 694/700\n",
      "1176/1176 [==============================] - 0s 70us/step - loss: 1.7916 - accuracy: 0.3257 - val_loss: 1.8113 - val_accuracy: 0.3194\n",
      "Epoch 695/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7919 - accuracy: 0.3265 - val_loss: 1.8090 - val_accuracy: 0.3175\n",
      "Epoch 696/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7912 - accuracy: 0.3291 - val_loss: 1.8087 - val_accuracy: 0.3155\n",
      "Epoch 697/700\n",
      "1176/1176 [==============================] - 0s 66us/step - loss: 1.7918 - accuracy: 0.3308 - val_loss: 1.8087 - val_accuracy: 0.3175\n",
      "Epoch 698/700\n",
      "1176/1176 [==============================] - 0s 69us/step - loss: 1.7913 - accuracy: 0.3333 - val_loss: 1.8078 - val_accuracy: 0.3175\n",
      "Epoch 699/700\n",
      "1176/1176 [==============================] - 0s 68us/step - loss: 1.7916 - accuracy: 0.3265 - val_loss: 1.8089 - val_accuracy: 0.3194\n",
      "Epoch 700/700\n",
      "1176/1176 [==============================] - 0s 67us/step - loss: 1.7912 - accuracy: 0.3274 - val_loss: 1.8117 - val_accuracy: 0.3194\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn = lstm, epochs = 700, batch_size = 200, verbose = 1)\n",
    "history=model.fit(X_train, train_targets, validation_data=(X_test,test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 35347 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 32244 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 33287 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 39511 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 35657 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30340 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25613 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 22833 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20989 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25976 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 35347 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 32244 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 33287 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 39511 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 35657 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30340 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25613 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 22833 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20989 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25976 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#繪圖\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss)+ 1)\n",
    "plt.plot(epochs, loss,'bo',label='Training loss')\n",
    "plt.plot(epochs, val_loss,'b',label='Validation loss')\n",
    "plt.title('訓練與驗證的損失函數')\n",
    "plt.xlabel('Epohs')\n",
    "plt.xlabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc = history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'r',label='Training acc')\n",
    "plt.plot(epochs, val_acc,'b',label='Validation acc')\n",
    "plt.title('TAT')\n",
    "plt.xlabel('Epohs')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試 小綠同學"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>serveTime</th>\n",
       "      <th>Loan</th>\n",
       "      <th>SalPerY</th>\n",
       "      <th>holdCard</th>\n",
       "      <th>Career</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>120</td>\n",
       "      <td>4</td>\n",
       "      <td>600000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age serveTime Loan SalPerY holdCard Career\n",
       "0   8       120    4  600000        1      1\n",
       "1  28        12    0  600000        0      0\n",
       "2  28        12    0      87        2      0"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "小綠 = pd.DataFrame(columns=[\"age\",\"serveTime\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\"])\n",
    "小綠.loc[0]=8,120,4,600000,1,1\n",
    "小綠.loc[1]=28,12,0,600000,0,0\n",
    "小綠.loc[2]=28,12,0,87,2,0\n",
    "小綠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#轉array\n",
    "小綠 = np.array(小綠).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1176, 6)\n",
      "(504, 6)\n",
      "(1176, 1)\n",
      "(504, 1)\n"
     ]
    }
   ],
   "source": [
    "#先打散資料(三次)\n",
    "for i in range(3):\n",
    "    df = shuffle(df)\n",
    "#再切成訓練與測試\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(df.loc[:, [\"age\",\"serveTime\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\"]] , df.loc[:, [\"credLimit_group\"]] , test_size=0.3, random_state=42)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_targets.shape)\n",
    "print(test_targets.shape)\n",
    "#轉array\n",
    "train_data = np.array(train_data).astype(float)\n",
    "test_data = np.array(test_data).astype(float)\n",
    "train_targets = np.array(train_targets).astype(int)\n",
    "test_targets = np.array(test_targets).astype(int)\n",
    "#把Y弄成onehot\n",
    "def to_one_hot(labels, dimension=20):\n",
    "    results = np.zeros((len(labels),dimension))\n",
    "    for i,label in enumerate(labels):\n",
    "        results[i,label]=1.\n",
    "    return results\n",
    "train_targets = to_one_hot(train_targets)\n",
    "test_targets = to_one_hot(test_targets)\n",
    "train_data_max = train_data.max(axis=0)\n",
    "train_data_min = train_data.min(axis=0)\n",
    "train_data_range = train_data_max-train_data_min\n",
    "小綠-=train_data_min\n",
    "小綠/=train_data_range\n",
    "#多一維\n",
    "小綠 = np.array(小綠).reshape((小綠.shape[0], 小綠.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 9ms/step\n",
      "一號小綠被分在第 0 群\n",
      "一號小綠被分在第 1 群\n",
      "一號小綠被分在第 4 群\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 他給出的是每一群的機率(相加為一)\n",
    "preds = model.predict(小綠)\n",
    "[print(\"一號小綠被分在第\",preds[i],\"群\") for i in range(len(小綠))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
